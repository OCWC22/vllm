#!/usr/bin/env python3
# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
"""
MAI-UI on T4 - Optimized Google Colab Notebook (CORRECTED)

This file can be run as a Python script or converted to a Colab notebook.
Copy the cells between the `# %%` markers into Colab.

GPU Requirements: NVIDIA T4 (16GB VRAM) - Available in free Colab tier

Architecture Overview:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    MAI-UI + vLLM + T4 ARCHITECTURE                       â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                          â”‚
    â”‚  Screenshot (1920Ã—1080)                                                  â”‚
    â”‚       â”‚                                                                  â”‚
    â”‚       â–¼                                                                  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚  IMAGE PREPROCESSING (max_pixels=512000)                         â”‚    â”‚
    â”‚  â”‚  â€¢ Resize to ~720Ã—720 max                                        â”‚    â”‚
    â”‚  â”‚  â€¢ Patchify: 14Ã—14 pixel patches                                 â”‚    â”‚
    â”‚  â”‚  â€¢ Merge: 2Ã—2 patches â†’ 1 token                                  â”‚    â”‚
    â”‚  â”‚  â€¢ Result: ~650 vision tokens                                    â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚       â”‚                                                                  â”‚
    â”‚       â–¼                                                                  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚  QWEN2-VL VISION ENCODER                                         â”‚    â”‚
    â”‚  â”‚  â€¢ TORCH_SDPA attention (T4 doesn't support FlashAttn2)         â”‚    â”‚
    â”‚  â”‚  â€¢ FP16 precision via Tensor Cores                               â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚       â”‚                                                                  â”‚
    â”‚       â–¼                                                                  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚  LANGUAGE MODEL (2B parameters)                                  â”‚    â”‚
    â”‚  â”‚  â€¢ PagedAttention for efficient KV cache                         â”‚    â”‚
    â”‚  â”‚  â€¢ Continuous batching for throughput                            â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚       â”‚                                                                  â”‚
    â”‚       â–¼                                                                  â”‚
    â”‚  Output: {"coordinate": [0.85, 0.12]}                                   â”‚
    â”‚                                                                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

# %% [markdown]
"""
# ğŸ–¥ï¸ MAI-UI on T4: Optimized GUI Agent with vLLM

This notebook demonstrates running MAI-UI (a vision-language model for GUI automation)
on Google Colab's free T4 GPU using vLLM's optimized inference engine.

## What You'll Learn
1. T4 GPU architecture and its limitations
2. vLLM's memory optimization techniques (PagedAttention)
3. How to configure vLLM for optimal T4 performance
4. Running GUI grounding inference with MAI-UI

## Requirements
- Google Colab with T4 GPU (free tier works!)
- ~15 minutes for first-time setup
"""

# %% [markdown]
"""
## ğŸ“‹ Cell 1: Check GPU and Install Dependencies
"""

# %%
# Cell 1: Setup and GPU Check
import subprocess
import sys

def run_cmd(cmd):
    """Run shell command and print output."""
    print(f"$ {cmd}")
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    if result.stdout:
        print(result.stdout)
    if result.stderr:
        print(result.stderr)
    return result.returncode == 0

# Check GPU
print("=" * 60)
print("ğŸ” GPU DETECTION")
print("=" * 60)
run_cmd("nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv")

# Check if we have a T4
import torch
if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    compute_cap = torch.cuda.get_device_capability()
    
    print(f"\nâœ… GPU: {gpu_name}")
    print(f"âœ… Memory: {gpu_memory:.1f} GB")
    print(f"âœ… Compute Capability: {compute_cap[0]}.{compute_cap[1]}")
    
    # T4 = SM 7.5, Ampere = SM 8.0+
    if "T4" in gpu_name:
        print("\nğŸ¯ T4 detected - using T4-optimized settings")
        IS_T4 = True
    elif compute_cap[0] >= 8:
        print("\nâœ¨ Ampere+ GPU - can use more aggressive settings")
        IS_T4 = False
    else:
        print("\nâš ï¸ Unknown GPU - using conservative settings")
        IS_T4 = True
else:
    print("âŒ No GPU detected! Please enable GPU in Runtime -> Change runtime type")
    sys.exit(1)

# Install vLLM
print("\n" + "=" * 60)
print("ğŸ“¦ INSTALLING DEPENDENCIES")
print("=" * 60)
run_cmd("pip install -q vllm>=0.6.0 pillow requests jinja2")
print("\nâœ… Dependencies installed!")

# %% [markdown]
"""
## ğŸ§  Cell 2: Understanding T4 Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        NVIDIA T4 GPU - TURING ARCHITECTURE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                  â”‚
â”‚  MEMORY: 16 GB GDDR6 @ 320 GB/s                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚   â”‚
â”‚  â”‚  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 16 GB Total â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                  â”‚
â”‚  COMPUTE: 40 SMs Ã— 64 CUDA Cores = 2,560 CUDA Cores                            â”‚
â”‚           40 SMs Ã— 8 Tensor Cores = 320 Tensor Cores (1st Gen)                  â”‚
â”‚                                                                                  â”‚
â”‚  PERFORMANCE:                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  FP32: 8.1 TFLOPS  â”‚  FP16: 65 TFLOPS  â”‚  INT8: 130 TOPS             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                                  â”‚
â”‚  T4 SUPPORTS:           T4 DOES NOT SUPPORT:                                    â”‚
â”‚  âœ… FP16 Tensor Cores   âŒ BF16 (requires Ampere+)                              â”‚
â”‚  âœ… INT8/INT4 Quant     âŒ FP8 (requires Hopper)                                â”‚
â”‚  âœ… PagedAttention      âŒ FlashAttention 2 (requires SM 8.0+)                  â”‚
â”‚  âœ… CUDA Graphs         âŒ Transformer Engine                                    â”‚
â”‚                                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
"""

# %% [markdown]
"""
## ğŸš€ Cell 3: Define T4-Optimized Configuration
"""

# %%
# Cell 3: T4-Optimized Configuration

# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚                        T4 MEMORY BUDGET (16 GB)                                 â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚  Component             â”‚  Size     â”‚  Percentage                               â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚  Model Weights (FP16)  â”‚  ~4.0 GB  â”‚  25%                                      â”‚
# â”‚  Vision Encoder Acts   â”‚  ~1.5 GB  â”‚  10%                                      â”‚
# â”‚  KV Cache              â”‚  ~4.0 GB  â”‚  25%                                      â”‚
# â”‚  Activations           â”‚  ~2.0 GB  â”‚  12%                                      â”‚
# â”‚  PyTorch/CUDA Overhead â”‚  ~1.5 GB  â”‚  10%                                      â”‚
# â”‚  Safety Headroom       â”‚  ~3.0 GB  â”‚  18%                                      â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# Configuration for MAI-UI-2B (Recommended for T4)
T4_CONFIG = {
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MODEL SETTINGS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "model": "Tongyi-MAI/MAI-UI-2B",    # âœ… CORRECT: Official MAI-UI model
    "trust_remote_code": True,           # âœ… REQUIRED: For custom Qwen2-VL code
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PRECISION & MEMORY
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "dtype": "half",                     # FP16 â†’ Tensor Core acceleration (65 TFLOPS)
    "gpu_memory_utilization": 0.90,      # Use 90% of VRAM, 10% safety margin
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CONTEXT & BATCHING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "max_model_len": 2048,               # âœ… Reduced for T4 (saves KV cache memory)
    "max_num_seqs": 4,                   # Max concurrent requests
    "enforce_eager": True,               # Disable CUDA graphs (saves ~500MB)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VISION SETTINGS (Critical for memory!)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "limit_mm_per_prompt": {"image": 1, "video": 0},  # One image per request
    "mm_processor_kwargs": {
        "min_pixels": 28 * 28,           # Minimum: 784 pixels (28Ã—28)
        "max_pixels": 512000,            # Maximum: ~720Ã—720 (saves ~30% tokens)
    },
}

print("=" * 60)
print("ğŸ“‹ T4-OPTIMIZED CONFIGURATION")
print("=" * 60)
for key, value in T4_CONFIG.items():
    print(f"  {key}: {value}")
print("=" * 60)

# %% [markdown]
"""
## ğŸ”§ Cell 4: Initialize vLLM Engine
"""

# %%
# Cell 4: Initialize vLLM Engine

from vllm import LLM, SamplingParams
import time

print("=" * 60)
print("ğŸš€ INITIALIZING vLLM ENGINE")
print("=" * 60)
print(f"\nModel: {T4_CONFIG['model']}")
print("This may take a few minutes on first run (downloading model)...\n")

init_start = time.time()

# Initialize LLM with T4-optimized config
llm = LLM(**T4_CONFIG)

init_time = time.time() - init_start
print(f"\nâœ… Engine initialized in {init_time:.1f} seconds")

# Print memory usage
if torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated() / (1024**3)
    reserved = torch.cuda.memory_reserved() / (1024**3)
    print(f"\nğŸ“Š GPU Memory Usage:")
    print(f"   Allocated: {allocated:.2f} GB")
    print(f"   Reserved:  {reserved:.2f} GB")
    print(f"   Free:      {16 - reserved:.2f} GB")

# %% [markdown]
"""
## ğŸ“¸ Cell 5: MAI-UI Prompt Format and Parsing
"""

# %%
# Cell 5: MAI-UI Prompt Format (CORRECT FORMAT)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAI-UI uses a SPECIFIC prompt format different from generic Qwen2-VL
# The grounding task expects:
#   Input:  <image> + instruction
#   Output: <grounding_think>reasoning</grounding_think><answer>{"coordinate":[x,y]}</answer>
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import re
import json

# MAI-UI Grounding System Prompt
MAI_GROUNDING_SYSTEM_PROMPT = """You are a GUI grounding agent. Given a screenshot and an instruction, locate the UI element described.

Output Format:
<grounding_think>
[Your reasoning about the element's location based on appearance, function, and position]
</grounding_think>
<answer>
{"coordinate": [x, y]}
</answer>

Coordinates are normalized to [0, 999] range where (0,0) is top-left and (999,999) is bottom-right."""


def build_mai_grounding_prompt(instruction: str) -> str:
    """
    Build prompt in MAI-UI's expected format.
    
    MAI-UI uses Qwen2-VL's ChatML format with vision tokens.
    """
    return (
        f"<|im_start|>system\n{MAI_GROUNDING_SYSTEM_PROMPT}<|im_end|>\n"
        f"<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>"
        f"{instruction}<|im_end|>\n"
        "<|im_start|>assistant\n"
    )


def parse_mai_grounding_response(text: str) -> dict:
    """
    Parse MAI-UI grounding response to extract coordinates.
    
    Handles the <grounding_think> and <answer> tags.
    """
    result = {
        "thinking": None,
        "coordinate": None,
        "coordinate_pixels": None,
        "raw": text,
    }
    
    # Extract thinking (reasoning)
    think_match = re.search(r"<grounding_think>(.*?)</grounding_think>", text, re.DOTALL)
    if think_match:
        result["thinking"] = think_match.group(1).strip()
    
    # Extract coordinate from <answer> tag
    answer_match = re.search(r"<answer>(.*?)</answer>", text, re.DOTALL)
    if answer_match:
        try:
            answer_json = json.loads(answer_match.group(1).strip())
            if "coordinate" in answer_json:
                # Normalize from [0, 999] to [0, 1]
                coord = answer_json["coordinate"]
                result["coordinate"] = [coord[0] / 999.0, coord[1] / 999.0]
        except json.JSONDecodeError:
            pass
    
    # Fallback: Try parsing pyautogui format for compatibility
    if result["coordinate"] is None:
        pyautogui_match = re.search(r"pyautogui\.click\((\d+),\s*(\d+)\)", text)
        if pyautogui_match:
            # This would be absolute pixels, not normalized
            x, y = int(pyautogui_match.group(1)), int(pyautogui_match.group(2))
            result["coordinate_pixels"] = [x, y]
    
    return result


print("âœ… MAI-UI prompt and parsing functions defined")
print("\nPrompt format preview:")
print("-" * 40)
print(build_mai_grounding_prompt("Click on the submit button")[:300] + "...")

# %% [markdown]
"""
## ğŸ“¸ Cell 6: Create Test Image
"""

# %%
# Cell 6: Create Test Image

from PIL import Image, ImageDraw

def create_test_screenshot():
    """Create a test screenshot simulating a mobile settings app."""
    width, height = 1080, 1920  # Mobile resolution
    img = Image.new('RGB', (width, height), color='#f5f5f5')
    draw = ImageDraw.Draw(img)
    
    # Status bar
    draw.rectangle([0, 0, width, 80], fill='#1976D2')
    draw.text((40, 30), "9:41", fill='white')
    draw.text((width - 100, 30), "100%", fill='white')
    
    # App bar
    draw.rectangle([0, 80, width, 200], fill='#2196F3')
    draw.text((40, 120), "Settings", fill='white')
    
    # Settings items
    items = [
        ("Wi-Fi", 280),
        ("Bluetooth", 400),
        ("Cellular", 520),
        ("Personal Hotspot", 640),
    ]
    
    for label, y in items:
        draw.rectangle([0, y, width, y + 100], fill='white', outline='#e0e0e0')
        draw.text((40, y + 35), label, fill='#333333')
        # Toggle switch
        draw.ellipse([width - 120, y + 30, width - 60, y + 70], fill='#4CAF50')
    
    # Bottom navigation
    draw.rectangle([0, height - 120, width, height], fill='white', outline='#e0e0e0')
    nav_items = ["Home", "Search", "Settings", "Profile"]
    for i, label in enumerate(nav_items):
        x = 60 + i * (width // 4)
        draw.text((x, height - 80), label, fill='#666666')
    
    return img


def create_desktop_screenshot():
    """Create a test screenshot simulating a desktop login form."""
    width, height = 1920, 1080
    img = Image.new('RGB', (width, height), color='#f0f0f0')
    draw = ImageDraw.Draw(img)
    
    # Title bar
    draw.rectangle([0, 0, width, 40], fill='#4a90d9')
    draw.text((20, 10), "Login - MyApp", fill='white')
    
    # Login form container
    form_x, form_y = 660, 300
    form_w, form_h = 600, 400
    draw.rectangle([form_x, form_y, form_x + form_w, form_y + form_h], 
                   fill='white', outline='#ccc')
    
    # Username field
    draw.text((form_x + 50, form_y + 50), "Username:", fill='#333')
    draw.rectangle([form_x + 50, form_y + 80, form_x + 550, form_y + 120], 
                   fill='white', outline='#999')
    
    # Password field
    draw.text((form_x + 50, form_y + 150), "Password:", fill='#333')
    draw.rectangle([form_x + 50, form_y + 180, form_x + 550, form_y + 220], 
                   fill='white', outline='#999')
    
    # Login button (target for clicking)
    btn_x1, btn_y1 = form_x + 50, form_y + 280
    btn_x2, btn_y2 = form_x + 200, form_y + 330
    draw.rectangle([btn_x1, btn_y1, btn_x2, btn_y2], fill='#4CAF50', outline='#45a049')
    draw.text((btn_x1 + 50, btn_y1 + 15), "Login", fill='white')
    
    # Cancel button
    draw.rectangle([form_x + 220, form_y + 280, form_x + 370, form_y + 330], 
                   fill='#f44336', outline='#da190b')
    draw.text((form_x + 260, form_y + 295), "Cancel", fill='white')
    
    return img


# Create both test images
test_image_mobile = create_test_screenshot()
test_image_desktop = create_desktop_screenshot()

# Use mobile for main tests (more representative of MAI-UI use case)
test_image = test_image_mobile
test_image.save("test_screenshot.png")
test_image_desktop.save("test_screenshot_desktop.png")

# Display the image
print("=" * 60)
print("ğŸ“¸ TEST SCREENSHOTS CREATED")
print("=" * 60)
print("\n1. Mobile settings app (test_screenshot.png)")
print(f"   Size: {test_image_mobile.size}")
print("   Contains: Wi-Fi, Bluetooth, Cellular, Navigation")
print("\n2. Desktop login form (test_screenshot_desktop.png)")
print(f"   Size: {test_image_desktop.size}")
print("   Contains: Login form with buttons")

# If in Colab, display the image
try:
    from IPython.display import display
    # Resize for display
    display_img = test_image.copy()
    display_img.thumbnail((300, 500))
    display(display_img)
except ImportError:
    print("\n(Images saved to test_screenshot.png)")

# %% [markdown]
"""
## ğŸ¤– Cell 7: Run Inference
"""

# %%
# Cell 7: Run MAI-UI Grounding Inference

# Sampling parameters (deterministic for reproducibility)
sampling_params = SamplingParams(
    temperature=0.0,       # Deterministic output
    max_tokens=512,        # Grounding needs short output
    stop=["<|im_end|>", "<|endoftext|>"],
)

# Test instructions
test_instructions = [
    "Click on Wi-Fi",
    "Click on Bluetooth",
    "Click on the Settings text in the app bar",
    "Click on the Home button in the navigation",
]

print("=" * 60)
print("ğŸ¤– MAI-UI GROUNDING INFERENCE")
print("=" * 60)

results = []
for i, instruction in enumerate(test_instructions, 1):
    print(f"\n[{i}] Instruction: \"{instruction}\"")
    
    # Prepare input
    prompt = build_mai_grounding_prompt(instruction)
    inputs = {
        "prompt": prompt,
        "multi_modal_data": {"image": test_image},
    }
    
    # Run inference
    start_time = time.time()
    outputs = llm.generate([inputs], sampling_params=sampling_params)
    latency = time.time() - start_time
    
    # Extract and parse result
    raw_output = outputs[0].outputs[0].text.strip()
    tokens = len(outputs[0].outputs[0].token_ids)
    parsed = parse_mai_grounding_response(raw_output)
    
    print(f"    Latency: {latency*1000:.0f}ms | Tokens: {tokens}")
    
    if parsed["coordinate"]:
        x, y = parsed["coordinate"]
        abs_x = int(x * test_image.width)
        abs_y = int(y * test_image.height)
        print(f"    Coordinate: [{x:.3f}, {y:.3f}] â†’ ({abs_x}, {abs_y}) pixels")
    elif parsed["coordinate_pixels"]:
        x, y = parsed["coordinate_pixels"]
        print(f"    Coordinate (pixels): ({x}, {y})")
    else:
        print(f"    Coordinate: Could not parse")
        print(f"    Raw output preview: {raw_output[:200]}...")
    
    if parsed["thinking"]:
        print(f"    Thinking: {parsed['thinking'][:100]}...")
    
    results.append({
        "instruction": instruction,
        "latency_ms": latency * 1000,
        "tokens": tokens,
        "parsed": parsed,
    })

print("\n" + "=" * 60)

# %% [markdown]
"""
## ğŸ¯ Cell 8: Visualize Results
"""

# %%
# Cell 8: Visualize Click Locations

def visualize_clicks(image, results):
    """Draw predicted click locations on image."""
    img = image.copy()
    draw = ImageDraw.Draw(img)
    
    colors = ['#FF0000', '#00FF00', '#0000FF', '#FF00FF', '#FFFF00', '#00FFFF']
    
    for i, result in enumerate(results):
        parsed = result["parsed"]
        coord = parsed.get("coordinate")
        coord_pixels = parsed.get("coordinate_pixels")
        
        if coord:
            x = int(coord[0] * image.width)
            y = int(coord[1] * image.height)
        elif coord_pixels:
            x, y = coord_pixels
        else:
            continue
        
        color = colors[i % len(colors)]
        
        # Draw circle marker
        r = 30
        draw.ellipse([x-r, y-r, x+r, y+r], outline=color, width=5)
        
        # Draw crosshair
        draw.line([x-r*1.5, y, x+r*1.5, y], fill=color, width=3)
        draw.line([x, y-r*1.5, x, y+r*1.5], fill=color, width=3)
        
        # Draw label number
        draw.text((x+r+10, y-10), str(i+1), fill=color)
    
    return img


print("=" * 60)
print("ğŸ¯ CLICK LOCATIONS VISUALIZED")
print("=" * 60)

vis_image = visualize_clicks(test_image, results)

try:
    from IPython.display import display
    # Resize for display
    vis_thumb = vis_image.copy()
    vis_thumb.thumbnail((300, 500))
    display(vis_thumb)
except ImportError:
    vis_image.save("results_visualization.png")
    print("Saved to results_visualization.png")

# Print legend
print("\nLegend:")
for i, result in enumerate(results, 1):
    parsed = result["parsed"]
    coord = parsed.get("coordinate")
    if coord:
        print(f"  [{i}] {result['instruction']}: ({coord[0]:.3f}, {coord[1]:.3f})")
    else:
        print(f"  [{i}] {result['instruction']}: (no coordinate)")

# %% [markdown]
"""
## ğŸ“Š Cell 9: Performance Benchmark
"""

# %%
# Cell 9: Benchmark Performance

import statistics

def benchmark_inference(llm, image, instruction, num_runs=5):
    """Benchmark inference performance."""
    prompt = build_mai_grounding_prompt(instruction)
    inputs = {"prompt": prompt, "multi_modal_data": {"image": image}}
    
    latencies = []
    for i in range(num_runs):
        start = time.time()
        outputs = llm.generate([inputs], sampling_params=sampling_params)
        latencies.append(time.time() - start)
    
    return {
        "mean_ms": statistics.mean(latencies) * 1000,
        "std_ms": statistics.stdev(latencies) * 1000 if len(latencies) > 1 else 0,
        "min_ms": min(latencies) * 1000,
        "max_ms": max(latencies) * 1000,
        "tokens": len(outputs[0].outputs[0].token_ids),
    }


print("=" * 60)
print("ğŸ“Š PERFORMANCE BENCHMARK")
print("=" * 60)
print("\nRunning 5 iterations for warm benchmark...\n")

bench_results = benchmark_inference(llm, test_image, "Click on Wi-Fi", num_runs=5)

# Calculate success rate from previous results
successful = sum(1 for r in results if r["parsed"]["coordinate"] or r["parsed"]["coordinate_pixels"])

print(f"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
print(f"â”‚  INFERENCE METRICS                                          â”‚")
print(f"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
print(f"â”‚  Requests Tested:   {len(results):<37}â”‚")
print(f"â”‚  Parse Success:     {successful}/{len(results)} ({successful/len(results)*100:.0f}%){'':>27}â”‚")
print(f"â”‚  Mean Latency:      {bench_results['mean_ms']:.0f} ms{'':>31}â”‚")
print(f"â”‚  Std Dev:           Â±{bench_results['std_ms']:.0f} ms{'':>30}â”‚")
print(f"â”‚  Min Latency:       {bench_results['min_ms']:.0f} ms{'':>31}â”‚")
print(f"â”‚  Max Latency:       {bench_results['max_ms']:.0f} ms{'':>31}â”‚")
print(f"â”‚  Tokens Generated:  {bench_results['tokens']:<37}â”‚")
print(f"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
print(f"â”‚  MEMORY USAGE                                               â”‚")
print(f"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")

if torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated() / (1024**3)
    reserved = torch.cuda.memory_reserved() / (1024**3)
    print(f"â”‚  Allocated:         {allocated:.2f} GB{'':>32}â”‚")
    print(f"â”‚  Reserved:          {reserved:.2f} GB{'':>32}â”‚")
    print(f"â”‚  T4 Total:          16.00 GB{'':>29}â”‚")
    print(f"â”‚  Utilization:       {reserved/16*100:.1f}%{'':>33}â”‚")

print(f"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

# %% [markdown]
"""
## ğŸ¯ Cell 10: Batch Inference Example
"""

# %%
# Cell 10: Batch Inference (vLLM Continuous Batching)

print("=" * 60)
print("ğŸ¯ BATCH INFERENCE (vLLM Continuous Batching)")
print("=" * 60)

# Prepare batch inputs using both test images
batch_tests = [
    {"image": test_image_mobile, "instruction": "Click on Bluetooth"},
    {"image": test_image_mobile, "instruction": "Click on the Profile button"},
    {"image": test_image_desktop, "instruction": "Click on the Login button"},
    {"image": test_image_desktop, "instruction": "Click on the Cancel button"},
]

batch_inputs = []
for test in batch_tests:
    prompt = build_mai_grounding_prompt(test["instruction"])
    batch_inputs.append({
        "prompt": prompt,
        "multi_modal_data": {"image": test["image"]},
    })

# Run batch inference
print(f"\nProcessing {len(batch_inputs)} requests in parallel...\n")

batch_start = time.time()
batch_outputs = llm.generate(batch_inputs, sampling_params=sampling_params)
batch_time = time.time() - batch_start

print(f"âœ… Batch completed in {batch_time:.2f}s")
print(f"ğŸ“ˆ Throughput: {len(batch_inputs) / batch_time:.2f} requests/second\n")

for i, (test, output) in enumerate(zip(batch_tests, batch_outputs), 1):
    raw_result = output.outputs[0].text.strip()
    parsed = parse_mai_grounding_response(raw_result)
    
    print(f"[{i}] {test['instruction']}")
    if parsed["coordinate"]:
        print(f"    â†’ Coordinate: [{parsed['coordinate'][0]:.3f}, {parsed['coordinate'][1]:.3f}]")
    else:
        print(f"    â†’ Raw: {raw_result[:100]}...")
    print()

# %% [markdown]
"""
## ğŸ“ Cell 11: Optimization Summary
"""

# %%
# Cell 11: Optimization Tips and Summary

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                           T4 OPTIMIZATION SUMMARY                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                                 â•‘
â•‘  WHAT WE OPTIMIZED:                                                             â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â•‘
â•‘  âœ… dtype=half           â†’ 8x faster via FP16 Tensor Cores                      â•‘
â•‘  âœ… max_model_len=2048   â†’ Limits KV cache to ~4GB                              â•‘
â•‘  âœ… enforce_eager=True   â†’ Saves 500MB by disabling CUDA graphs                 â•‘
â•‘  âœ… max_pixels=512000    â†’ ~30% fewer vision tokens                             â•‘
â•‘  âœ… max_num_seqs=4       â†’ Limits concurrent memory usage                       â•‘
â•‘  âœ… trust_remote_code    â†’ Required for custom Qwen2-VL model code              â•‘
â•‘                                                                                 â•‘
â•‘  T4 LIMITATIONS (What We Can't Change):                                         â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â•‘
â•‘  âŒ No FlashAttention 2 (uses TORCH_SDPA instead)                               â•‘
â•‘  âŒ No BF16 support (FP16 only)                                                 â•‘
â•‘  âŒ No FP8 quantization (Hopper only)                                           â•‘
â•‘  âŒ 320 GB/s bandwidth â†’ decode phase is memory-bound                           â•‘
â•‘                                                                                 â•‘
â•‘  IF YOU GET OOM ERRORS:                                                         â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â•‘
â•‘  1. Reduce max_model_len â†’ 1024                                                 â•‘
â•‘  2. Reduce max_pixels â†’ 256000                                                  â•‘
â•‘  3. Reduce gpu_memory_utilization â†’ 0.85                                        â•‘
â•‘  4. Reduce max_num_seqs â†’ 2                                                     â•‘
â•‘                                                                                 â•‘
â•‘  FOR BETTER QUALITY (if you have more GPU memory):                              â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â•‘
â•‘  1. Increase max_pixels â†’ 768000 or 1003520 (default)                           â•‘
â•‘  2. Increase max_model_len â†’ 4096                                               â•‘
â•‘  3. Use MAI-UI-8B with BitsAndBytes 4-bit quantization                          â•‘
â•‘                                                                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

print("\n" + "=" * 60)
print("âœ… NOTEBOOK COMPLETE")
print("=" * 60)
print("""
ğŸ‰ You have successfully run MAI-UI on T4 with vLLM!

Next Steps:
1. Try with your own screenshots
2. Integrate with pyautogui for actual GUI automation
3. Deploy as an API server (see server.py)

Repository: https://github.com/OCWC22/vllm/tree/main/examples/mai_ui_t4
""")
