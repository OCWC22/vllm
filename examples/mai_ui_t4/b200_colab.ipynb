{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# âš¡ MAI-UI on B200: Next-Gen Blackwell Performance\n",
        "\n",
        "This notebook runs MAI-UI on NVIDIA B200 GPU (Blackwell architecture).\n",
        "\n",
        "**B200 Advantages:**\n",
        "- 192GB HBM3e (12x more VRAM than T4)\n",
        "- 8.0 TB/s bandwidth (25x faster than T4)\n",
        "- FP4 Tensor Cores (4x throughput vs FP16)\n",
        "- 5th Gen Tensor Cores\n",
        "- 2nd Gen Transformer Engine\n",
        "\n",
        "**Note:** B200 features are bleeding-edge. Some vLLM optimizations may require updates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Check GPU and Install Dependencies\n",
        "import subprocess, sys\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ğŸ” GPU DETECTION\")\n",
        "print(\"=\" * 70)\n",
        "!nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    cc = torch.cuda.get_device_capability()\n",
        "    \n",
        "    print(f\"\\nâœ… GPU: {gpu_name}\")\n",
        "    print(f\"âœ… Memory: {gpu_memory:.1f} GB\")\n",
        "    print(f\"âœ… Compute Capability: SM {cc[0]}.{cc[1]}\")\n",
        "    \n",
        "    if \"B200\" in gpu_name or \"B100\" in gpu_name or cc[0] >= 10:\n",
        "        print(\"\\nâš¡ Blackwell detected - using next-gen settings!\")\n",
        "    elif cc[0] >= 9:\n",
        "        print(\"\\nğŸš€ Hopper detected - using H100 settings (close to B200)\")\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸ Expected B200, got {gpu_name}\")\n",
        "else:\n",
        "    print(\"âŒ No GPU!\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"\\nğŸ“¦ Installing dependencies...\")\n",
        "%pip install -q vllm>=0.6.0 pillow requests jinja2\n",
        "print(\"âœ… Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: B200-Optimized Configuration\n",
        "from vllm import LLM, SamplingParams\n",
        "import time\n",
        "\n",
        "# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "# â”‚                        B200 MEMORY BUDGET (192 GB)                          â”‚\n",
        "# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "# â”‚  Component             â”‚  Size      â”‚  Notes                               â”‚\n",
        "# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "# â”‚  Model Weights (BF16)  â”‚  ~16 GB    â”‚  8B params Ã— 2 bytes                 â”‚\n",
        "# â”‚  KV Cache              â”‚  ~100 GB   â”‚  128K context Ã— 128 seqs             â”‚\n",
        "# â”‚  Activations           â”‚  ~30 GB    â”‚  Massive batch support               â”‚\n",
        "# â”‚  CUDA Graphs           â”‚  ~5 GB     â”‚  Pre-compiled kernels                â”‚\n",
        "# â”‚  Prefix Cache          â”‚  ~30 GB    â”‚  Reusable KV blocks                  â”‚\n",
        "# â”‚  Headroom              â”‚  ~11 GB    â”‚  Safety margin                       â”‚\n",
        "# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "B200_CONFIG = {\n",
        "    \"model\": \"Tongyi-MAI/MAI-UI-8B\",  # Can run 8B at full precision!\n",
        "    \"trust_remote_code\": True,\n",
        "    \"dtype\": \"bfloat16\",\n",
        "    \"gpu_memory_utilization\": 0.95,\n",
        "    \"max_model_len\": 65536,  # 64K context (vs 2K on T4)\n",
        "    \"max_num_seqs\": 128,  # 128 concurrent (vs 4 on T4)\n",
        "    \"enforce_eager\": False,  # CUDA Graphs enabled\n",
        "    \"limit_mm_per_prompt\": {\"image\": 16, \"video\": 4},  # Multiple 4K images\n",
        "    \"mm_processor_kwargs\": {\"min_pixels\": 784, \"max_pixels\": 4147200},  # 4K resolution\n",
        "    \"enable_prefix_caching\": True,\n",
        "    \"enable_chunked_prefill\": True,\n",
        "}\n",
        "\n",
        "print(\"âš¡ B200-OPTIMIZED CONFIGURATION\")\n",
        "print(\"=\" * 70)\n",
        "for k, v in B200_CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Initialize vLLM Engine\n",
        "print(\"âš¡ Initializing vLLM with B200 optimizations...\")\n",
        "print(\"   - FlashAttention (latest version)\")\n",
        "print(\"   - FP4 quantization (when available)\")\n",
        "print(\"   - 64K context, 128 concurrent sequences\\n\")\n",
        "\n",
        "init_start = time.time()\n",
        "llm = LLM(**B200_CONFIG)\n",
        "init_time = time.time() - init_start\n",
        "\n",
        "print(f\"\\nâœ… Engine initialized in {init_time:.1f}s\")\n",
        "\n",
        "allocated = torch.cuda.memory_allocated() / (1024**3)\n",
        "reserved = torch.cuda.memory_reserved() / (1024**3)\n",
        "print(f\"\\nğŸ“Š GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
        "print(f\"ğŸ“Š Available: {192 - reserved:.1f} GB free\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: MAI-UI Prompt Format\n",
        "import re, json\n",
        "\n",
        "MAI_PROMPT = \"\"\"You are a GUI grounding agent. Locate the UI element described.\n",
        "Output: <grounding_think>[reasoning]</grounding_think><answer>{\"coordinate\": [x, y]}</answer>\"\"\"\n",
        "\n",
        "def build_prompt(instruction):\n",
        "    return (f\"<|im_start|>system\\n{MAI_PROMPT}<|im_end|>\\n\"\n",
        "            f\"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>{instruction}<|im_end|>\\n\"\n",
        "            \"<|im_start|>assistant\\n\")\n",
        "\n",
        "def parse_response(text):\n",
        "    result = {\"thinking\": None, \"coordinate\": None}\n",
        "    think = re.search(r\"<grounding_think>(.*?)</grounding_think>\", text, re.DOTALL)\n",
        "    if think: result[\"thinking\"] = think.group(1).strip()\n",
        "    answer = re.search(r\"<answer>(.*?)</answer>\", text, re.DOTALL)\n",
        "    if answer:\n",
        "        try:\n",
        "            data = json.loads(answer.group(1).strip())\n",
        "            if \"coordinate\" in data:\n",
        "                result[\"coordinate\"] = [data[\"coordinate\"][0]/999, data[\"coordinate\"][1]/999]\n",
        "        except: pass\n",
        "    return result\n",
        "\n",
        "print(\"âœ… Prompt functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Create 4K Test Image\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def create_4k_screenshot():\n",
        "    \"\"\"Create 4K test screenshot (3840x2160).\"\"\"\n",
        "    img = Image.new('RGB', (3840, 2160), '#0a0a1a')\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    \n",
        "    # Top bar\n",
        "    draw.rectangle([0, 0, 3840, 100], fill='#1a1a3a')\n",
        "    draw.text((40, 35), \"Enterprise Dashboard - 4K Resolution\", fill='#eee')\n",
        "    \n",
        "    # Sidebar\n",
        "    draw.rectangle([0, 100, 400, 2160], fill='#0f1f3f')\n",
        "    for i, item in enumerate([\"Dashboard\", \"Analytics\", \"Reports\", \"Users\", \"Settings\", \"Admin\", \"Logs\"]):\n",
        "        draw.text((50, 150 + i*100), f\"ğŸ“Š {item}\", fill='#c0c0c0')\n",
        "    \n",
        "    # Main content - large cards\n",
        "    for row in range(2):\n",
        "        for col in range(4):\n",
        "            x, y = 450 + col*800, 150 + row*500\n",
        "            draw.rectangle([x, y, x+750, y+450], fill='#1f2f4f', outline='#3a5a8a')\n",
        "            draw.text((x+30, y+30), f\"Metric {row*4 + col + 1}\", fill='#888')\n",
        "            draw.text((x+30, y+100), f\"${(row*4 + col + 1) * 12345:,}\", fill='#00ff88')\n",
        "    \n",
        "    # Action buttons\n",
        "    for i, (text, color) in enumerate([(\"Export All\", '#4a90d9'), (\"Generate Report\", '#e94560'),\n",
        "                                        (\"Sync Data\", '#0f9b58'), (\"Configure\", '#ff9800')]):\n",
        "        x = 450 + i*400\n",
        "        draw.rectangle([x, 1150, x+350, 1220], fill=color)\n",
        "        draw.text((x+50, 1170), text, fill='white')\n",
        "    \n",
        "    return img\n",
        "\n",
        "test_image = create_4k_screenshot()\n",
        "print(f\"ğŸ“¸ Created 4K image: {test_image.size}\")\n",
        "thumb = test_image.copy(); thumb.thumbnail((800, 450)); display(thumb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Run Inference\n",
        "sampling_params = SamplingParams(temperature=0.0, max_tokens=512, stop=[\"<|im_end|>\"])\n",
        "\n",
        "instructions = [\"Click on Analytics\", \"Click on Export All\", \"Click on Metric 1\", \n",
        "                \"Click on Settings\", \"Click on Generate Report\", \"Click on Admin\"]\n",
        "\n",
        "print(\"âš¡ Running B200-optimized inference...\")\n",
        "results = []\n",
        "for i, inst in enumerate(instructions, 1):\n",
        "    inputs = {\"prompt\": build_prompt(inst), \"multi_modal_data\": {\"image\": test_image}}\n",
        "    start = time.time()\n",
        "    outputs = llm.generate([inputs], sampling_params=sampling_params)\n",
        "    latency = (time.time() - start) * 1000\n",
        "    parsed = parse_response(outputs[0].outputs[0].text)\n",
        "    results.append({\"instruction\": inst, \"latency_ms\": latency, \"parsed\": parsed})\n",
        "    coord = parsed.get(\"coordinate\", \"N/A\")\n",
        "    print(f\"[{i}] {inst:30} â†’ {str(coord):20} {latency:.0f}ms\")\n",
        "\n",
        "avg = sum(r[\"latency_ms\"] for r in results) / len(results)\n",
        "print(f\"\\nğŸ“Š Average latency: {avg:.0f}ms (vs ~1000ms on T4 = {1000/avg:.1f}x faster)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Massive Batch Inference (B200 Maximum Throughput)\n",
        "print(\"âš¡ MASSIVE BATCH INFERENCE - B200 Maximum Throughput\")\n",
        "\n",
        "# Generate 64 instructions\n",
        "batch_insts = [f\"Click on Metric {i+1}\" for i in range(8)] + \\\n",
        "              [f\"Click on {item}\" for item in [\"Dashboard\", \"Analytics\", \"Reports\", \"Users\", \n",
        "                                                 \"Settings\", \"Admin\", \"Logs\", \"Export All\",\n",
        "                                                 \"Generate Report\", \"Sync Data\", \"Configure\"]] * 5\n",
        "\n",
        "batch_inputs = [{\"prompt\": build_prompt(i), \"multi_modal_data\": {\"image\": test_image}} \n",
        "                for i in batch_insts[:64]]\n",
        "\n",
        "print(f\"\\nProcessing {len(batch_inputs)} requests in parallel...\")\n",
        "batch_start = time.time()\n",
        "batch_outputs = llm.generate(batch_inputs, sampling_params=sampling_params)\n",
        "batch_time = time.time() - batch_start\n",
        "\n",
        "print(f\"\\nâœ… Completed in {batch_time:.2f}s\")\n",
        "print(f\"ğŸ“ˆ Throughput: {len(batch_inputs) / batch_time:.1f} requests/second\")\n",
        "print(f\"ğŸ“ˆ Per-request: {batch_time / len(batch_inputs) * 1000:.0f}ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: GPU Architecture Comparison Summary\n",
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                        GPU ARCHITECTURE COMPARISON                               â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘                                                                                 â•‘\n",
        "â•‘  Metric              â”‚  T4 (Turing)  â”‚  H100 (Hopper)  â”‚  B200 (Blackwell)     â•‘\n",
        "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â•‘\n",
        "â•‘  VRAM                â”‚  16 GB        â”‚  80 GB          â”‚  192 GB               â•‘\n",
        "â•‘  Bandwidth           â”‚  320 GB/s     â”‚  3,350 GB/s     â”‚  8,000 GB/s           â•‘\n",
        "â•‘  FP16 TFLOPS         â”‚  65           â”‚  1,979          â”‚  ~4,000               â•‘\n",
        "â•‘  FP8 TFLOPS          â”‚  âŒ           â”‚  3,958          â”‚  ~8,000               â•‘\n",
        "â•‘  FP4 TFLOPS          â”‚  âŒ           â”‚  âŒ             â”‚  ~16,000              â•‘\n",
        "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â•‘\n",
        "â•‘  Expected Latency    â”‚  ~1000ms      â”‚  ~200ms         â”‚  ~100ms               â•‘\n",
        "â•‘  Expected Throughput â”‚  ~1 req/s     â”‚  ~6 req/s       â”‚  ~15 req/s            â•‘\n",
        "â•‘  Max Concurrent      â”‚  4            â”‚  64             â”‚  128                  â•‘\n",
        "â•‘  Max Context         â”‚  2K           â”‚  32K            â”‚  128K                 â•‘\n",
        "â•‘  Max Resolution      â”‚  720Ã—720      â”‚  1920Ã—1080      â”‚  3840Ã—2160 (4K)       â•‘\n",
        "â•‘                                                                                 â•‘\n",
        "â•‘  UNIQUE B200 FEATURES:                                                          â•‘\n",
        "â•‘  âœ… FP4 Tensor Cores (4x throughput when available)                             â•‘\n",
        "â•‘  âœ… 2nd Gen Transformer Engine                                                   â•‘\n",
        "â•‘  âœ… HBM3e (25x bandwidth vs T4)                                                  â•‘\n",
        "â•‘  âœ… NVLink 5.0 (1.8 TB/s multi-GPU)                                              â•‘\n",
        "â•‘  âœ… Decompression Engine                                                         â•‘\n",
        "â•‘                                                                                 â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")\n",
        "print(\"âš¡ B200 NOTEBOOK COMPLETE!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
