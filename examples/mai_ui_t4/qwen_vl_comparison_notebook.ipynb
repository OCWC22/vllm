{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ Qwen2-VL vs Qwen3-VL: Complete Architecture Deep Dive\n",
        "\n",
        "This notebook provides comprehensive coverage of:\n",
        "1. **Architecture Deep Dive** - Vision encoder, position encoding, activation functions\n",
        "2. **GPU-Optimized Configurations** - T4, A100, H100, B200 specific settings\n",
        "3. **DeepStack & EVS** - Qwen3-VL exclusive features explained\n",
        "4. **Production-Ready Inference** - Optimized code for each GPU tier\n",
        "\n",
        "---\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "```\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                              QWEN2-VL vs QWEN3-VL ARCHITECTURE EVOLUTION                          â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘                                                                                                   â•‘\n",
        "â•‘  QWEN2-VL (2024)                                    QWEN3-VL (2025)                               â•‘\n",
        "â•‘  â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                     â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                â•‘\n",
        "â•‘                                                                                                   â•‘\n",
        "â•‘  Image/Video â”€â”€â–¶ Conv3D (NO BIAS)                   Image/Video â”€â”€â–¶ Conv3D (WITH BIAS)            â•‘\n",
        "â•‘        â”‚                                                  â”‚                                       â•‘\n",
        "â•‘        â–¼                                                  â–¼                                       â•‘\n",
        "â•‘  3D RoPE Position Encoding                          Learned + Interpolated Position Embed         â•‘\n",
        "â•‘        â”‚                                                  â”‚                                       â•‘\n",
        "â•‘        â–¼                                                  â–¼                                       â•‘\n",
        "â•‘  Vision Transformer Blocks                          Vision Transformer Blocks â”€â”€â”                 â•‘\n",
        "â•‘  â€¢ LayerNorm                                        â€¢ LayerNorm                 â”‚ DeepStack       â•‘\n",
        "â•‘  â€¢ Attention (QKV + RoPE)                           â€¢ Attention (QKV + RoPE)    â”‚ Mergers at      â•‘\n",
        "â•‘  â€¢ MLP (QuickGELU, WITH bias)                       â€¢ MLP (SiLU, NO bias)       â”‚ intermediate    â•‘\n",
        "â•‘        â”‚                                                  â”‚                     â”‚ layers          â•‘\n",
        "â•‘        â–¼                                                  â–¼                     â”‚                 â•‘\n",
        "â•‘  Single Patch Merger                                Main Merger + DeepStack â—„â”€â”€â”€â”˜                 â•‘\n",
        "â•‘        â”‚                                                  â”‚                                       â•‘\n",
        "â•‘        â–¼                                                  â–¼                                       â•‘\n",
        "â•‘  Qwen2 LLM Backbone                                 Qwen3 LLM with DeepStack Injection            â•‘\n",
        "â•‘  (visual tokens at embed level)                     (multi-scale features in early layers)        â•‘\n",
        "â•‘                                                                                                   â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "```\n",
        "\n",
        "## Key Architectural Differences Explained\n",
        "\n",
        "### 1. Patch Embedding (Conv3D)\n",
        "- **Qwen2-VL**: `bias=False` - Relies on LayerNorm for centering\n",
        "- **Qwen3-VL**: `bias=True` - Learnable offset captures low-frequency patterns\n",
        "\n",
        "### 2. Position Encoding\n",
        "- **Qwen2-VL**: Pure 3D RoPE from grid coordinates\n",
        "- **Qwen3-VL**: Learned embeddings + bilinear interpolation for any resolution + partial RoPE\n",
        "\n",
        "### 3. MLP Activation\n",
        "- **Qwen2-VL**: QuickGELU `x * Ïƒ(1.702x)` - Fast approximation\n",
        "- **Qwen3-VL**: SiLU `x * Ïƒ(x)` - Smoother gradients, modern standard\n",
        "\n",
        "### 4. DeepStack (Qwen3-VL Only)\n",
        "- Extracts features from intermediate vision layers\n",
        "- Injects multi-scale features into early LLM layers\n",
        "- Better fine-grained visual understanding\n",
        "\n",
        "### 5. EVS - Efficient Video Sampling (Qwen3-VL Only)\n",
        "- Content-aware token pruning based on frame similarity\n",
        "- Reduces video tokens by 30-70% with minimal quality loss\n",
        "- Enables processing of much longer videos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Architecture Impact on Qwen VL Models\n",
        "\n",
        "### Why GPU Architecture Matters\n",
        "\n",
        "Each GPU architecture has specific capabilities that affect how Qwen VL models run:\n",
        "\n",
        "| GPU | VRAM | Bandwidth | Key Features | Best For |\n",
        "|-----|------|-----------|--------------|----------|\n",
        "| **T4** | 16GB | 320 GB/s | FP16, SDPA only | Qwen2-VL-2B, Qwen3-VL-4B (4-bit) |\n",
        "| **A100** | 40/80GB | 2.0 TB/s | BF16, FlashAttn v2 | Qwen2/3-VL 7B/8B full precision |\n",
        "| **H100** | 80GB | 3.35 TB/s | FP8, FlashAttn v3 | All models + FP8 quantization |\n",
        "| **B200** | 192GB | 8.0 TB/s | FP4 (future), 128K ctx | Qwen2-VL-72B single GPU! |\n",
        "\n",
        "### Attention Backend Selection\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Backend         â”‚ T4 (SM 7.5) â”‚ A100 (SM 8.0) â”‚ H100 (SM 9.0) â”‚ B200      â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  FLASH_ATTN      â”‚ âŒ          â”‚ âœ…            â”‚ âœ… (v3)       â”‚ âœ…        â”‚\n",
        "â”‚  TORCH_SDPA      â”‚ âœ… (only)   â”‚ âœ…            â”‚ âœ…            â”‚ âœ…        â”‚\n",
        "â”‚  XFORMERS        â”‚ âš ï¸ Qwen2    â”‚ âš ï¸ Qwen2      â”‚ âš ï¸ Qwen2      â”‚ âš ï¸ Qwen2  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "Note: Qwen3-VL only supports FLASH_ATTN, TORCH_SDPA, and ROCM_AITER_FA\n",
        "```\n",
        "\n",
        "### Memory Budget Examples\n",
        "\n",
        "**T4 (16GB) with Qwen2-VL-2B:**\n",
        "- Model weights: ~5 GB\n",
        "- KV Cache (2K ctx): ~2 GB  \n",
        "- Activations: ~1 GB\n",
        "- CUDA context: ~1.5 GB\n",
        "- Vision encoder temp: ~2 GB\n",
        "- **Available for batch: ~4.5 GB**\n",
        "\n",
        "**H100 (80GB) with Qwen3-VL-8B:**\n",
        "- Model weights: ~17 GB\n",
        "- KV Cache (32K ctx): ~25 GB\n",
        "- Activations: ~8 GB\n",
        "- DeepStack buffers: ~2 GB\n",
        "- Prefix cache: ~15 GB\n",
        "- **Available for batch: ~13 GB**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: GPU Detection & Environment Setup\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ” GPU DETECTION & ENVIRONMENT SETUP\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Check GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    cc = torch.cuda.get_device_capability()\n",
        "    \n",
        "    print(f\"\\nâœ… GPU: {gpu_name}\")\n",
        "    print(f\"âœ… Memory: {gpu_memory:.1f} GB\")\n",
        "    print(f\"âœ… Compute Capability: SM {cc[0]}.{cc[1]}\")\n",
        "    \n",
        "    # Determine architecture and recommendations\n",
        "    if cc[0] >= 10:\n",
        "        arch = \"Blackwell (B200)\"\n",
        "        recommended = \"Qwen3-VL-8B (128K context) or Qwen2-VL-72B\"\n",
        "    elif cc[0] >= 9:\n",
        "        arch = \"Hopper (H100)\"\n",
        "        recommended = \"Qwen3-VL-8B or Qwen3-VL-30B-A3B (MoE)\"\n",
        "    elif cc[0] >= 8:\n",
        "        arch = \"Ampere (A100)\"\n",
        "        recommended = \"Qwen3-VL-8B or Qwen2-VL-7B\"\n",
        "    else:\n",
        "        arch = \"Turing (T4)\"\n",
        "        recommended = \"Qwen2-VL-2B or Qwen3-VL-4B (4-bit)\"\n",
        "    \n",
        "    print(f\"âœ… Architecture: {arch}\")\n",
        "    print(f\"âœ… Recommended: {recommended}\")\n",
        "    \n",
        "    GPU_INFO = {\"name\": gpu_name, \"memory\": gpu_memory, \"sm\": cc[0]}\n",
        "else:\n",
        "    print(\"âŒ No GPU available!\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"\\nğŸ“¦ Installing dependencies...\")\n",
        "%pip install -q vllm>=0.6.0 pillow requests numpy\n",
        "print(\"âœ… Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: GPU-Optimized Configuration Selection with Detailed Documentation\n",
        "\n",
        "def get_optimal_config(sm_version: int, vram_gb: float, prefer_qwen3: bool = True):\n",
        "    \"\"\"\n",
        "    Get optimal vLLM configuration based on GPU architecture.\n",
        "    \n",
        "    Architecture-Specific Optimizations:\n",
        "    ====================================\n",
        "    \n",
        "    T4 (SM 7.5, 16GB GDDR6, 320 GB/s):\n",
        "    - No FlashAttention v2 (requires SM 8.0+) â†’ Uses TORCH_SDPA\n",
        "    - No BF16 support â†’ Must use FP16 (dtype=\"half\")\n",
        "    - No FP8 quantization â†’ Use bitsandbytes INT4 for larger models\n",
        "    - Limited memory â†’ enforce_eager=True saves ~500MB CUDA graphs\n",
        "    - Low bandwidth â†’ Reduce max_pixels to minimize vision processing\n",
        "    \n",
        "    A100 (SM 8.0, 80GB HBM2e, 2.0 TB/s):\n",
        "    - FlashAttention v2 supported â†’ Significant speedup\n",
        "    - BF16 native support â†’ Better numerical stability\n",
        "    - 6x T4 bandwidth â†’ Can handle larger images/more batching\n",
        "    - Use prefix caching for repeated prompts\n",
        "    \n",
        "    H100 (SM 9.0, 80GB HBM3, 3.35 TB/s):\n",
        "    - FlashAttention v3 (Hopper-optimized)\n",
        "    - FP8 hardware support â†’ 2x throughput when quantized\n",
        "    - 10x T4 bandwidth â†’ Ultra-fast vision processing\n",
        "    - Transformer Engine for automatic mixed precision\n",
        "    \n",
        "    B200 (SM 10.0, 192GB HBM3e, 8.0 TB/s):\n",
        "    - FP4 hardware support (future, ~4x throughput)\n",
        "    - 25x T4 bandwidth â†’ Massive throughput\n",
        "    - Huge memory â†’ 128K context, 4K images, or 72B models\n",
        "    \n",
        "    Args:\n",
        "        sm_version: CUDA Compute Capability major version (7=Turing, 8=Ampere, etc.)\n",
        "        vram_gb: GPU memory in GB\n",
        "        prefer_qwen3: Whether to prefer Qwen3-VL over Qwen2-VL\n",
        "        \n",
        "    Returns:\n",
        "        dict: vLLM configuration optimized for the GPU\n",
        "    \"\"\"\n",
        "    \n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    # T4 / TURING (SM 7.5, 16GB)\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    if sm_version < 8:\n",
        "        # Architecture constraints:\n",
        "        # - No FlashAttention v2 (falls back to TORCH_SDPA for vision encoder)\n",
        "        # - No BF16 tensor cores (must use FP16)\n",
        "        # - Limited to ~14GB usable (CUDA context ~1.5GB)\n",
        "        \n",
        "        if prefer_qwen3:\n",
        "            return {\n",
        "                \"model\": \"Qwen/Qwen3-VL-4B-Instruct\",\n",
        "                \"trust_remote_code\": True,\n",
        "                \"dtype\": \"half\",  # T4 doesn't support BF16\n",
        "                \n",
        "                # Quantization: Required for 4B model on 16GB\n",
        "                \"quantization\": \"bitsandbytes\",\n",
        "                \"load_format\": \"bitsandbytes\",\n",
        "                \n",
        "                # Memory optimization\n",
        "                \"gpu_memory_utilization\": 0.92,  # Leave some headroom\n",
        "                \"max_model_len\": 2048,  # Limited context for memory\n",
        "                \"enforce_eager\": True,  # Save ~500MB by disabling CUDA graphs\n",
        "                \n",
        "                # Batch size limited due to memory\n",
        "                \"max_num_seqs\": 4,\n",
        "                \n",
        "                # Media limits - conservative for T4\n",
        "                \"limit_mm_per_prompt\": {\"image\": 1, \"video\": 1},\n",
        "                \"mm_processor_kwargs\": {\n",
        "                    \"min_pixels\": 784,  # 28Ã—28 minimum\n",
        "                    \"max_pixels\": 512000,  # ~720Ã—720 max\n",
        "                    \"video_pruning_rate\": 0.5,  # EVS: Prune 50% for Qwen3-VL\n",
        "                },\n",
        "                \n",
        "                # Disable advanced features that need memory\n",
        "                \"enable_prefix_caching\": False,\n",
        "                \"enable_chunked_prefill\": False,\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"model\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
        "                \"trust_remote_code\": True,\n",
        "                \"dtype\": \"half\",  # FP16 only\n",
        "                \n",
        "                # No quantization needed for 2B\n",
        "                \"gpu_memory_utilization\": 0.90,\n",
        "                \"max_model_len\": 2048,\n",
        "                \"enforce_eager\": True,  # Save memory\n",
        "                \"max_num_seqs\": 4,\n",
        "                \n",
        "                # Qwen2-VL: No video support on T4 (too slow)\n",
        "                \"limit_mm_per_prompt\": {\"image\": 1, \"video\": 0},\n",
        "                \"mm_processor_kwargs\": {\n",
        "                    \"min_pixels\": 784,\n",
        "                    \"max_pixels\": 512000,\n",
        "                },\n",
        "                \n",
        "                \"enable_prefix_caching\": False,\n",
        "                \"enable_chunked_prefill\": False,\n",
        "            }\n",
        "    \n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    # A100 / AMPERE (SM 8.0, 40-80GB)\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    elif sm_version < 9:\n",
        "        # Architecture features:\n",
        "        # - FlashAttention v2 supported (significant speedup)\n",
        "        # - BF16 native tensor core support\n",
        "        # - 2.0 TB/s bandwidth (6x T4)\n",
        "        # - Up to 80GB HBM2e memory\n",
        "        \n",
        "        if prefer_qwen3:\n",
        "            return {\n",
        "                \"model\": \"Qwen/Qwen3-VL-8B-Instruct\",\n",
        "                \"trust_remote_code\": True,\n",
        "                \"dtype\": \"bfloat16\",  # Use BF16 for better stability\n",
        "                \n",
        "                # Leverage abundant memory\n",
        "                \"gpu_memory_utilization\": 0.95,\n",
        "                \"max_model_len\": 16384,  # 16K context\n",
        "                \"enforce_eager\": False,  # Enable CUDA graphs\n",
        "                \"max_num_seqs\": 32,  # Good concurrency\n",
        "                \n",
        "                # Can handle multiple images/videos\n",
        "                \"limit_mm_per_prompt\": {\"image\": 8, \"video\": 4},\n",
        "                \"mm_processor_kwargs\": {\n",
        "                    \"min_pixels\": 784,\n",
        "                    \"max_pixels\": 1572864,  # ~1280Ã—1280\n",
        "                    \"video_pruning_rate\": 0.3,  # EVS: 30% pruning\n",
        "                },\n",
        "                \n",
        "                # Enable advanced features\n",
        "                \"enable_prefix_caching\": True,  # Leverage memory\n",
        "                \"enable_chunked_prefill\": True,  # Better long-sequence handling\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"model\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
        "                \"trust_remote_code\": True,\n",
        "                \"dtype\": \"bfloat16\",\n",
        "                \"gpu_memory_utilization\": 0.95,\n",
        "                \"max_model_len\": 16384,\n",
        "                \"enforce_eager\": False,\n",
        "                \"max_num_seqs\": 32,\n",
        "                \"limit_mm_per_prompt\": {\"image\": 8, \"video\": 2},  # Less video (no EVS)\n",
        "                \"mm_processor_kwargs\": {\n",
        "                    \"min_pixels\": 784,\n",
        "                    \"max_pixels\": 1572864,\n",
        "                },\n",
        "                \"enable_prefix_caching\": True,\n",
        "                \"enable_chunked_prefill\": True,\n",
        "            }\n",
        "    \n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    # H100 / HOPPER (SM 9.0, 80GB HBM3)\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    elif sm_version < 10:\n",
        "        # Architecture features:\n",
        "        # - FlashAttention v3 (Hopper-optimized kernels)\n",
        "        # - FP8 hardware support (2x throughput)\n",
        "        # - 3.35 TB/s bandwidth (10x T4!)\n",
        "        # - Transformer Engine for automatic mixed precision\n",
        "        \n",
        "        if prefer_qwen3:\n",
        "            return {\n",
        "                \"model\": \"Qwen/Qwen3-VL-8B-Instruct\",\n",
        "                \"trust_remote_code\": True,\n",
        "                \"dtype\": \"bfloat16\",\n",
        "                \n",
        "                # H100 can handle 32K context easily\n",
        "                \"gpu_memory_utilization\": 0.95,\n",
        "                \"max_model_len\": 32768,  # 32K context\n",
        "                \"enforce_eager\": False,\n",
        "                \"max_num_seqs\": 64,  # High concurrency\n",
        "                \n",
        "                # Handle many images/videos\n",
        "                \"limit_mm_per_prompt\": {\"image\": 16, \"video\": 8},\n",
        "                \"mm_processor_kwargs\": {\n",
        "                    \"min_pixels\": 784,\n",
        "                    \"max_pixels\": 2073600,  # 1920Ã—1080 (1080p)\n",
        "                    \"video_pruning_rate\": 0.3,  # EVS\n",
        "                },\n",
        "                \n",
        "                \"enable_prefix_caching\": True,\n",
        "                \"enable_chunked_prefill\": True,\n",
        "                \n",
        "                # Optional: Add for maximum throughput\n",
        "                # \"quantization\": \"fp8\",  # 2x throughput\n",
        "                # \"kv_cache_dtype\": \"fp8\",  # 2x KV capacity\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"model\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
        "                \"trust_remote_code\": True,\n",
        "                \"dtype\": \"bfloat16\",\n",
        "                \"gpu_memory_utilization\": 0.95,\n",
        "                \"max_model_len\": 32768,\n",
        "                \"enforce_eager\": False,\n",
        "                \"max_num_seqs\": 64,\n",
        "                \"limit_mm_per_prompt\": {\"image\": 16, \"video\": 4},\n",
        "                \"mm_processor_kwargs\": {\n",
        "                    \"min_pixels\": 784,\n",
        "                    \"max_pixels\": 2073600,\n",
        "                },\n",
        "                \"enable_prefix_caching\": True,\n",
        "                \"enable_chunked_prefill\": True,\n",
        "            }\n",
        "    \n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    # B200 / BLACKWELL (SM 10.0, 192GB HBM3e)\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    else:\n",
        "        # Architecture features:\n",
        "        # - FP4 hardware support (future, ~4x throughput)\n",
        "        # - 8.0 TB/s bandwidth (25x T4!)\n",
        "        # - 192GB HBM3e memory\n",
        "        # - 2nd Gen Transformer Engine\n",
        "        # - Can run Qwen2-VL-72B on a SINGLE GPU!\n",
        "        \n",
        "        if prefer_qwen3:\n",
        "            return {\n",
        "                \"model\": \"Qwen/Qwen3-VL-8B-Instruct\",\n",
        "                \"trust_remote_code\": True,\n",
        "                \"dtype\": \"bfloat16\",\n",
        "                \n",
        "                # Ultra-long context possible!\n",
        "                \"gpu_memory_utilization\": 0.95,\n",
        "                \"max_model_len\": 131072,  # 128K context!\n",
        "                \"enforce_eager\": False,\n",
        "                \"max_num_seqs\": 128,  # Massive batch\n",
        "                \n",
        "                # Handle many high-res images/videos\n",
        "                \"limit_mm_per_prompt\": {\"image\": 32, \"video\": 16},\n",
        "                \"mm_processor_kwargs\": {\n",
        "                    \"min_pixels\": 784,\n",
        "                    \"max_pixels\": 4147200,  # 4K resolution!\n",
        "                    \"video_pruning_rate\": 0.3,  # EVS\n",
        "                },\n",
        "                \n",
        "                \"enable_prefix_caching\": True,\n",
        "                \"enable_chunked_prefill\": True,\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"model\": \"Qwen/Qwen2-VL-72B-Instruct\",  # FULL 72B on single GPU!\n",
        "                \"trust_remote_code\": True,\n",
        "                \"dtype\": \"bfloat16\",\n",
        "                \"gpu_memory_utilization\": 0.95,\n",
        "                \"max_model_len\": 32768,  # 32K with 72B model\n",
        "                \"enforce_eager\": False,\n",
        "                \"max_num_seqs\": 32,\n",
        "                \"limit_mm_per_prompt\": {\"image\": 16, \"video\": 8},\n",
        "                \"mm_processor_kwargs\": {\n",
        "                    \"min_pixels\": 784,\n",
        "                    \"max_pixels\": 4147200,  # 4K\n",
        "                },\n",
        "                \"enable_prefix_caching\": True,\n",
        "                \"enable_chunked_prefill\": True,\n",
        "            }\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Configuration Selection\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Set PREFER_QWEN3 = False to use Qwen2-VL instead\n",
        "PREFER_QWEN3 = True\n",
        "\n",
        "config = get_optimal_config(GPU_INFO[\"sm\"], GPU_INFO[\"memory\"], prefer_qwen3=PREFER_QWEN3)\n",
        "\n",
        "print(\"âš¡ SELECTED CONFIGURATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"   GPU: {GPU_INFO['name']} ({GPU_INFO['memory']:.1f} GB, SM {GPU_INFO['sm']})\")\n",
        "print(\"=\" * 80)\n",
        "for k, v in config.items():\n",
        "    print(f\"   {k}: {v}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Display configuration rationale\n",
        "if GPU_INFO['sm'] < 8:\n",
        "    print(\"\\nğŸ“ T4 Optimizations Applied:\")\n",
        "    print(\"   â€¢ Using TORCH_SDPA (no FlashAttention v2 on SM 7.x)\")\n",
        "    print(\"   â€¢ FP16 dtype (no BF16 tensor cores)\")\n",
        "    print(\"   â€¢ enforce_eager=True (saves ~500MB CUDA memory)\")\n",
        "    print(\"   â€¢ max_pixels=512000 (~720Ã—720 to fit in memory)\")\n",
        "    if PREFER_QWEN3:\n",
        "        print(\"   â€¢ bitsandbytes INT4 quantization for Qwen3-VL-4B\")\n",
        "        print(\"   â€¢ EVS video_pruning_rate=0.5 (50% token reduction)\")\n",
        "elif GPU_INFO['sm'] < 9:\n",
        "    print(\"\\nğŸ“ A100 Optimizations Applied:\")\n",
        "    print(\"   â€¢ FlashAttention v2 enabled (SM 8.0)\")\n",
        "    print(\"   â€¢ BF16 dtype (tensor core optimized)\")\n",
        "    print(\"   â€¢ Prefix caching enabled (leverage 80GB memory)\")\n",
        "    if PREFER_QWEN3:\n",
        "        print(\"   â€¢ EVS video_pruning_rate=0.3 (30% token reduction)\")\n",
        "elif GPU_INFO['sm'] < 10:\n",
        "    print(\"\\nğŸ“ H100 Optimizations Applied:\")\n",
        "    print(\"   â€¢ FlashAttention v3 (Hopper-optimized)\")\n",
        "    print(\"   â€¢ 32K context enabled\")\n",
        "    print(\"   â€¢ Consider FP8 quantization for 2x throughput\")\n",
        "    if PREFER_QWEN3:\n",
        "        print(\"   â€¢ EVS video_pruning_rate=0.3 (30% token reduction)\")\n",
        "else:\n",
        "    print(\"\\nğŸ“ B200 Optimizations Applied:\")\n",
        "    print(\"   â€¢ 128K context enabled (with Qwen3-VL)\")\n",
        "    print(\"   â€¢ 4K image resolution supported\")\n",
        "    print(\"   â€¢ Massive batch size (128 sequences)\")\n",
        "    if not PREFER_QWEN3:\n",
        "        print(\"   â€¢ Running Qwen2-VL-72B on SINGLE GPU!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Initialize vLLM Engine\n",
        "from vllm import LLM, SamplingParams\n",
        "import time\n",
        "\n",
        "print(\"âš¡ Initializing vLLM engine...\")\n",
        "print(f\"   Model: {config['model']}\")\n",
        "\n",
        "init_start = time.time()\n",
        "llm = LLM(**config)\n",
        "init_time = time.time() - init_start\n",
        "\n",
        "print(f\"\\nâœ… Engine initialized in {init_time:.1f}s\")\n",
        "\n",
        "allocated = torch.cuda.memory_allocated() / (1024**3)\n",
        "reserved = torch.cuda.memory_reserved() / (1024**3)\n",
        "print(f\"\\nğŸ“Š GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Create Test Image\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def create_test_image(width=1024, height=1024):\n",
        "    \"\"\"Create test dashboard image.\"\"\"\n",
        "    img = Image.new('RGB', (width, height), '#0f1420')\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    \n",
        "    # Top bar\n",
        "    draw.rectangle([0, 0, width, 60], fill='#1a2535')\n",
        "    draw.text((20, 20), \"Dashboard - Analytics\", fill='#ffffff')\n",
        "    \n",
        "    # Sidebar\n",
        "    draw.rectangle([0, 60, 200, height], fill='#141d2b')\n",
        "    items = [\"Dashboard\", \"Analytics\", \"Reports\", \"Settings\", \"Users\"]\n",
        "    for i, item in enumerate(items):\n",
        "        y = 100 + i * 50\n",
        "        draw.rectangle([10, y, 190, y + 40], fill='#1f2d42')\n",
        "        draw.text((30, y + 10), item, fill='#b0b0b0')\n",
        "    \n",
        "    # Cards\n",
        "    for row in range(2):\n",
        "        for col in range(3):\n",
        "            x, y = 220 + col * 260, 80 + row * 200\n",
        "            draw.rectangle([x, y, x + 240, y + 180], fill='#1a2535', outline='#2a3a4f')\n",
        "            draw.text((x + 20, y + 20), f\"Metric {row * 3 + col + 1}\", fill='#808080')\n",
        "            draw.text((x + 20, y + 60), f\"${(row * 3 + col + 1) * 1234:,}\", fill='#00d084')\n",
        "    \n",
        "    # Buttons\n",
        "    for i, (text, color) in enumerate([(\"Export\", '#3a7bd5'), (\"Refresh\", '#00a86b')]):\n",
        "        x = 220 + i * 150\n",
        "        draw.rectangle([x, height - 80, x + 130, height - 40], fill=color)\n",
        "        draw.text((x + 20, height - 70), text, fill='#ffffff')\n",
        "    \n",
        "    return img\n",
        "\n",
        "test_image = create_test_image()\n",
        "print(f\"ğŸ“¸ Created test image: {test_image.size}\")\n",
        "\n",
        "# Display thumbnail\n",
        "thumb = test_image.copy()\n",
        "thumb.thumbnail((600, 600))\n",
        "display(thumb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Prompt Format Function\n",
        "def format_prompt(instruction: str, model_name: str) -> str:\n",
        "    \"\"\"Format prompt based on model family.\"\"\"\n",
        "    system = \"You are a helpful visual assistant. Analyze images carefully.\"\n",
        "    \n",
        "    return (\n",
        "        f\"<|im_start|>system\\n{system}<|im_end|>\\n\"\n",
        "        f\"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>{instruction}<|im_end|>\\n\"\n",
        "        \"<|im_start|>assistant\\n\"\n",
        "    )\n",
        "\n",
        "print(\"âœ… Prompt functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Run Single Image Inference\n",
        "sampling_params = SamplingParams(temperature=0.0, max_tokens=512, stop=[\"<|im_end|>\"])\n",
        "\n",
        "instructions = [\n",
        "    \"Describe this dashboard interface.\",\n",
        "    \"What menu items are visible in the sidebar?\",\n",
        "    \"What is the total value of all metrics shown?\",\n",
        "]\n",
        "\n",
        "print(\"âš¡ Running inference...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "model_name = config[\"model\"]\n",
        "results = []\n",
        "\n",
        "for i, instruction in enumerate(instructions, 1):\n",
        "    prompt = format_prompt(instruction, model_name)\n",
        "    inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"image\": test_image}}\n",
        "    \n",
        "    start = time.time()\n",
        "    outputs = llm.generate([inputs], sampling_params=sampling_params)\n",
        "    latency = (time.time() - start) * 1000\n",
        "    \n",
        "    response = outputs[0].outputs[0].text\n",
        "    tokens = len(outputs[0].outputs[0].token_ids)\n",
        "    \n",
        "    print(f\"\\n[{i}] {instruction}\")\n",
        "    print(f\"    â±ï¸  Latency: {latency:.0f}ms | Tokens: {tokens} | Speed: {tokens/(latency/1000):.1f} tok/s\")\n",
        "    print(f\"    ğŸ“ Response: {response[:200]}...\" if len(response) > 200 else f\"    ğŸ“ Response: {response}\")\n",
        "    \n",
        "    results.append({\"instruction\": instruction, \"latency\": latency, \"tokens\": tokens})\n",
        "\n",
        "avg_latency = sum(r[\"latency\"] for r in results) / len(results)\n",
        "print(f\"\\nğŸ“Š Average latency: {avg_latency:.0f}ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Batch Inference (High Throughput)\n",
        "batch_instructions = [\n",
        "    \"Click on Dashboard\", \"Click on Analytics\", \"Click on Reports\",\n",
        "    \"Click on Settings\", \"Click on Users\", \"Click on Export\",\n",
        "    \"Click on Refresh\", \"Click on Metric 1\",\n",
        "]\n",
        "\n",
        "batch_inputs = [\n",
        "    {\"prompt\": format_prompt(inst, model_name), \"multi_modal_data\": {\"image\": test_image}}\n",
        "    for inst in batch_instructions\n",
        "]\n",
        "\n",
        "print(f\"âš¡ Batch inference: {len(batch_inputs)} requests\")\n",
        "\n",
        "batch_start = time.time()\n",
        "batch_outputs = llm.generate(batch_inputs, sampling_params=sampling_params)\n",
        "batch_time = (time.time() - batch_start) * 1000\n",
        "\n",
        "print(f\"\\nâœ… Completed in {batch_time:.0f}ms\")\n",
        "print(f\"ğŸ“ˆ Throughput: {len(batch_inputs) / (batch_time / 1000):.1f} requests/second\")\n",
        "print(f\"ğŸ“ˆ Per-request: {batch_time / len(batch_inputs):.0f}ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Complete Architecture & GPU Summary\n",
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                        COMPLETE QWEN VL ARCHITECTURE COMPARISON                                   â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘                                                                                                   â•‘\n",
        "â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•‘\n",
        "â•‘  â”‚ Component              â”‚ Qwen2-VL                    â”‚ Qwen3-VL                            â”‚   â•‘\n",
        "â•‘  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â•‘\n",
        "â•‘  â”‚ VISION ENCODER                                                                             â”‚   â•‘\n",
        "â•‘  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚   â•‘\n",
        "â•‘  â”‚ Patch Embedding        â”‚ Conv3D bias=False           â”‚ Conv3D bias=True                    â”‚   â•‘\n",
        "â•‘  â”‚ Position Encoding      â”‚ 3D RoPE only                â”‚ Learned + Bilinear Interp + RoPE    â”‚   â•‘\n",
        "â•‘  â”‚ MLP Activation         â”‚ QuickGELU: x*Ïƒ(1.702x)      â”‚ SiLU: x*Ïƒ(x)                        â”‚   â•‘\n",
        "â•‘  â”‚ MLP Bias               â”‚ Has bias                    â”‚ No bias (linear_fc1/fc2)            â”‚   â•‘\n",
        "â•‘  â”‚ Feature Extraction     â”‚ Final layer only            â”‚ Multi-scale (DeepStack)             â”‚   â•‘\n",
        "â•‘  â”‚ Video Max Frames       â”‚ 14 frames                   â”‚ 24,576 frames                       â”‚   â•‘\n",
        "â•‘  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â•‘\n",
        "â•‘  â”‚ LLM INTEGRATION                                                                            â”‚   â•‘\n",
        "â•‘  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚   â•‘\n",
        "â•‘  â”‚ Backbone               â”‚ Qwen2ForCausalLM            â”‚ Qwen3ForCausalLM                    â”‚   â•‘\n",
        "â•‘  â”‚ Visual Injection       â”‚ Embedding level only        â”‚ Embed + Early layer injection       â”‚   â•‘\n",
        "â•‘  â”‚ DeepStack              â”‚ âŒ Not supported            â”‚ âœ… Multi-scale feature injection    â”‚   â•‘\n",
        "â•‘  â”‚ EVS (Video Pruning)    â”‚ âŒ Not supported            â”‚ âœ… 30-70% token reduction           â”‚   â•‘\n",
        "â•‘  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â•‘\n",
        "â•‘  â”‚ OPTIMIZATION                                                                               â”‚   â•‘\n",
        "â•‘  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚   â•‘\n",
        "â•‘  â”‚ Attention Backends     â”‚ FlashAttn, SDPA, Xformers   â”‚ FlashAttn, SDPA only                â”‚   â•‘\n",
        "â•‘  â”‚ Torch Compile          â”‚ Limited                     â”‚ @support_torch_compile              â”‚   â•‘\n",
        "â•‘  â”‚ Eagle3 Speculative     â”‚ âŒ Not supported            â”‚ âœ… SupportsEagle3 interface         â”‚   â•‘\n",
        "â•‘  â”‚ Multimodal Pruning     â”‚ âŒ Not supported            â”‚ âœ… SupportsMultiModalPruning        â”‚   â•‘\n",
        "â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•‘\n",
        "â•‘                                                                                                   â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                              GPU-SPECIFIC OPTIMIZATION GUIDE                                      â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘                                                                                                   â•‘\n",
        "â•‘  T4 (TURING SM 7.5, 16GB GDDR6, 320 GB/s)                                                         â•‘\n",
        "â•‘  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                        â•‘\n",
        "â•‘  Constraints: No FlashAttn v2, No BF16, No FP8, Limited memory                                    â•‘\n",
        "â•‘  Recommended: enforce_eager=True, max_num_seqs=4, max_pixels=512000                               â•‘\n",
        "â•‘  Best models: Qwen2-VL-2B (FP16), Qwen3-VL-4B (4-bit with EVS)                                    â•‘\n",
        "â•‘  Expected: ~800-1200ms latency, ~15-25 tok/s                                                      â•‘\n",
        "â•‘                                                                                                   â•‘\n",
        "â•‘  A100 (AMPERE SM 8.0, 80GB HBM2e, 2.0 TB/s)                                                       â•‘\n",
        "â•‘  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                        â•‘\n",
        "â•‘  Features: FlashAttn v2, BF16, Tensor Cores                                                       â•‘\n",
        "â•‘  Recommended: dtype=bfloat16, max_num_seqs=32, enable_prefix_caching=True                         â•‘\n",
        "â•‘  Best models: Qwen2-VL-7B, Qwen3-VL-8B, Qwen3-VL-30B-A3B (MoE)                                    â•‘\n",
        "â•‘  Expected: ~200-400ms latency, ~80-120 tok/s                                                      â•‘\n",
        "â•‘                                                                                                   â•‘\n",
        "â•‘  H100 (HOPPER SM 9.0, 80GB HBM3, 3.35 TB/s)                                                       â•‘\n",
        "â•‘  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                        â•‘\n",
        "â•‘  Features: FlashAttn v3, FP8, Transformer Engine, 10x T4 bandwidth                                â•‘\n",
        "â•‘  Recommended: FP8 quant for max throughput, kv_cache_dtype=fp8                                    â•‘\n",
        "â•‘  Best models: Qwen3-VL-8B (32K ctx), Qwen3-VL-8B FP8 (64K ctx!)                                   â•‘\n",
        "â•‘  Expected: ~100-200ms latency, ~120-180 tok/s                                                     â•‘\n",
        "â•‘                                                                                                   â•‘\n",
        "â•‘  B200 (BLACKWELL SM 10.0, 192GB HBM3e, 8.0 TB/s)                                                  â•‘\n",
        "â•‘  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                        â•‘\n",
        "â•‘  Features: FP4 (future), 25x T4 bandwidth, massive memory                                         â•‘\n",
        "â•‘  Recommended: max_model_len=131072 (128K!), max_pixels=4147200 (4K)                               â•‘\n",
        "â•‘  Best models: Qwen3-VL-8B (128K ctx), Qwen2-VL-72B (single GPU!)                                  â•‘\n",
        "â•‘  Expected: ~50-100ms latency, ~180-300 tok/s                                                      â•‘\n",
        "â•‘                                                                                                   â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                                  DEEPSTACK EXPLAINED                                              â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘                                                                                                   â•‘\n",
        "â•‘  DeepStack extracts features from INTERMEDIATE vision layers and injects them into EARLY          â•‘\n",
        "â•‘  language model layers. This provides multi-resolution visual information to the LLM.             â•‘\n",
        "â•‘                                                                                                   â•‘\n",
        "â•‘  Vision Encoder:                                                                                  â•‘\n",
        "â•‘    Layer 0  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º              â•‘\n",
        "â•‘    ...                                                                                            â•‘\n",
        "â•‘    Layer k  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º DeepStack Merger [0] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â•‘\n",
        "â•‘    ...                                                          â”‚                                  â•‘\n",
        "â•‘    Layer m  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º DeepStack Merger [1] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”                                â•‘\n",
        "â•‘    ...                                                          â”‚ â”‚                                â•‘\n",
        "â•‘    Layer N  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Main Merger â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚                                â•‘\n",
        "â•‘                                                              â”‚  â”‚ â”‚                                â•‘\n",
        "â•‘                                  CONCATENATE: [main | ds0 | ds1]                                  â•‘\n",
        "â•‘                                              â–¼                                                     â•‘\n",
        "â•‘  LLM Backbone:                                                                                    â•‘\n",
        "â•‘    Layer 0: hidden += deepstack_embeds[0]  â—„â”€â”€ Early injection                                    â•‘\n",
        "â•‘    Layer 1: hidden += deepstack_embeds[1]  â—„â”€â”€ Early injection                                    â•‘\n",
        "â•‘    Layer 2: normal forward                                                                        â•‘\n",
        "â•‘    ...                                                                                            â•‘\n",
        "â•‘                                                                                                   â•‘\n",
        "â•‘  BENEFIT: LLM has access to low-level (edges), mid-level (shapes), and high-level (semantics)    â•‘\n",
        "â•‘           visual features throughout its processing, not just at the input embedding level.       â•‘\n",
        "â•‘                                                                                                   â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                            EVS (EFFICIENT VIDEO SAMPLING) EXPLAINED                               â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘                                                                                                   â•‘\n",
        "â•‘  EVS uses content-aware pruning based on frame-to-frame similarity:                               â•‘\n",
        "â•‘                                                                                                   â•‘\n",
        "â•‘  Algorithm:                                                                                       â•‘\n",
        "â•‘    1. Compute cosine similarity between consecutive frames                                        â•‘\n",
        "â•‘    2. dissimilarity = 1 - similarity (higher = more different = more important)                  â•‘\n",
        "â•‘    3. First frame always kept (dissimilarity set to max)                                          â•‘\n",
        "â•‘    4. Sort all tokens by dissimilarity, keep top (1-q) fraction                                   â•‘\n",
        "â•‘    5. Recompute M-RoPE positions for retained tokens                                              â•‘\n",
        "â•‘                                                                                                   â•‘\n",
        "â•‘  Example (60s video, 120 frames):                                                                 â•‘\n",
        "â•‘    Without EVS: ~19,200 tokens â†’ may exceed context window!                                       â•‘\n",
        "â•‘    With EVS (q=0.5): ~9,600 tokens â†’ 2x faster, fits in context                                   â•‘\n",
        "â•‘                                                                                                   â•‘\n",
        "â•‘  Configuration:                                                                                   â•‘\n",
        "â•‘    mm_processor_kwargs = {\"video_pruning_rate\": 0.5}  # Prune 50%                                 â•‘\n",
        "â•‘                                                                                                   â•‘\n",
        "â•‘  Quality vs Speed tradeoff:                                                                       â•‘\n",
        "â•‘    q=0.3 (30% pruning): Minimal quality loss, 1.4x speedup                                        â•‘\n",
        "â•‘    q=0.5 (50% pruning): Slight quality loss, 2x speedup                                           â•‘\n",
        "â•‘    q=0.7 (70% pruning): Noticeable loss, 3x speedup                                               â•‘\n",
        "â•‘                                                                                                   â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")\n",
        "\n",
        "print(\"âœ… Notebook complete!\")\n",
        "print(\"\\\\nğŸ“š For more details, see: qwen_vl_deep_dive.md\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
