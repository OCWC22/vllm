{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ Qwen2-VL vs Qwen3-VL: Complete Comparison & Optimized Inference\n",
        "\n",
        "This notebook provides:\n",
        "1. Architecture comparison between Qwen2-VL and Qwen3-VL\n",
        "2. GPU-optimized configurations for T4, A100, H100, and B200\n",
        "3. Production-ready inference code\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    QWEN2-VL vs QWEN3-VL KEY DIFFERENCES                             â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚  Feature              â”‚ Qwen2-VL                  â”‚ Qwen3-VL                        â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚\n",
        "â”‚  Position Encoding    â”‚ 3D RoPE only              â”‚ Learned + RoPE + Interpolate    â”‚\n",
        "â”‚  MLP Activation       â”‚ QuickGELU                 â”‚ SiLU (configurable)             â”‚\n",
        "â”‚  Multi-Scale Features â”‚ âŒ                        â”‚ âœ… DeepStack                    â”‚\n",
        "â”‚  Video Token Pruning  â”‚ âŒ                        â”‚ âœ… EVS (Efficient Video Samp)   â”‚\n",
        "â”‚  Max Video Frames     â”‚ 14                        â”‚ 24,576                          â”‚\n",
        "â”‚  Speculative Decode   â”‚ Basic                     â”‚ âœ… Eagle3 native                â”‚\n",
        "â”‚  MoE Variants         â”‚ âŒ                        â”‚ âœ… Qwen3-VL-30B-A3B             â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: GPU Detection & Environment Setup\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ” GPU DETECTION & ENVIRONMENT SETUP\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Check GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    cc = torch.cuda.get_device_capability()\n",
        "    \n",
        "    print(f\"\\nâœ… GPU: {gpu_name}\")\n",
        "    print(f\"âœ… Memory: {gpu_memory:.1f} GB\")\n",
        "    print(f\"âœ… Compute Capability: SM {cc[0]}.{cc[1]}\")\n",
        "    \n",
        "    # Determine architecture and recommendations\n",
        "    if cc[0] >= 10:\n",
        "        arch = \"Blackwell (B200)\"\n",
        "        recommended = \"Qwen3-VL-8B (128K context) or Qwen2-VL-72B\"\n",
        "    elif cc[0] >= 9:\n",
        "        arch = \"Hopper (H100)\"\n",
        "        recommended = \"Qwen3-VL-8B or Qwen3-VL-30B-A3B (MoE)\"\n",
        "    elif cc[0] >= 8:\n",
        "        arch = \"Ampere (A100)\"\n",
        "        recommended = \"Qwen3-VL-8B or Qwen2-VL-7B\"\n",
        "    else:\n",
        "        arch = \"Turing (T4)\"\n",
        "        recommended = \"Qwen2-VL-2B or Qwen3-VL-4B (4-bit)\"\n",
        "    \n",
        "    print(f\"âœ… Architecture: {arch}\")\n",
        "    print(f\"âœ… Recommended: {recommended}\")\n",
        "    \n",
        "    GPU_INFO = {\"name\": gpu_name, \"memory\": gpu_memory, \"sm\": cc[0]}\n",
        "else:\n",
        "    print(\"âŒ No GPU available!\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"\\nğŸ“¦ Installing dependencies...\")\n",
        "%pip install -q vllm>=0.6.0 pillow requests numpy\n",
        "print(\"âœ… Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: GPU-Optimized Configuration Selection\n",
        "\n",
        "def get_optimal_config(sm_version: int, vram_gb: float, prefer_qwen3: bool = True):\n",
        "    \"\"\"Get optimal vLLM configuration based on GPU.\"\"\"\n",
        "    \n",
        "    # T4 / TURING (SM 7.5, 16GB)\n",
        "    if sm_version < 8:\n",
        "        if prefer_qwen3:\n",
        "            return {\n",
        "                \"model\": \"Qwen/Qwen3-VL-4B-Instruct\",\n",
        "                \"trust_remote_code\": True, \"dtype\": \"half\",\n",
        "                \"quantization\": \"bitsandbytes\", \"load_format\": \"bitsandbytes\",\n",
        "                \"gpu_memory_utilization\": 0.92, \"max_model_len\": 2048,\n",
        "                \"enforce_eager\": True, \"max_num_seqs\": 4,\n",
        "                \"limit_mm_per_prompt\": {\"image\": 1, \"video\": 1},\n",
        "                \"mm_processor_kwargs\": {\"min_pixels\": 784, \"max_pixels\": 512000},\n",
        "                \"enable_prefix_caching\": False, \"enable_chunked_prefill\": False,\n",
        "            }\n",
        "        return {\n",
        "            \"model\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
        "            \"trust_remote_code\": True, \"dtype\": \"half\",\n",
        "            \"gpu_memory_utilization\": 0.90, \"max_model_len\": 2048,\n",
        "            \"enforce_eager\": True, \"max_num_seqs\": 4,\n",
        "            \"limit_mm_per_prompt\": {\"image\": 1, \"video\": 0},\n",
        "            \"mm_processor_kwargs\": {\"min_pixels\": 784, \"max_pixels\": 512000},\n",
        "            \"enable_prefix_caching\": False, \"enable_chunked_prefill\": False,\n",
        "        }\n",
        "    \n",
        "    # A100 / AMPERE (SM 8.0, 40-80GB)\n",
        "    elif sm_version < 9:\n",
        "        if prefer_qwen3:\n",
        "            return {\n",
        "                \"model\": \"Qwen/Qwen3-VL-8B-Instruct\",\n",
        "                \"trust_remote_code\": True, \"dtype\": \"bfloat16\",\n",
        "                \"gpu_memory_utilization\": 0.95, \"max_model_len\": 16384,\n",
        "                \"enforce_eager\": False, \"max_num_seqs\": 32,\n",
        "                \"limit_mm_per_prompt\": {\"image\": 8, \"video\": 4},\n",
        "                \"mm_processor_kwargs\": {\"min_pixels\": 784, \"max_pixels\": 1572864},\n",
        "                \"enable_prefix_caching\": True, \"enable_chunked_prefill\": True,\n",
        "            }\n",
        "        return {\n",
        "            \"model\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
        "            \"trust_remote_code\": True, \"dtype\": \"bfloat16\",\n",
        "            \"gpu_memory_utilization\": 0.95, \"max_model_len\": 16384,\n",
        "            \"enforce_eager\": False, \"max_num_seqs\": 32,\n",
        "            \"limit_mm_per_prompt\": {\"image\": 8, \"video\": 2},\n",
        "            \"mm_processor_kwargs\": {\"min_pixels\": 784, \"max_pixels\": 1572864},\n",
        "            \"enable_prefix_caching\": True, \"enable_chunked_prefill\": True,\n",
        "        }\n",
        "    \n",
        "    # H100 / HOPPER (SM 9.0, 80GB HBM3)\n",
        "    elif sm_version < 10:\n",
        "        if prefer_qwen3:\n",
        "            return {\n",
        "                \"model\": \"Qwen/Qwen3-VL-8B-Instruct\",\n",
        "                \"trust_remote_code\": True, \"dtype\": \"bfloat16\",\n",
        "                \"gpu_memory_utilization\": 0.95, \"max_model_len\": 32768,\n",
        "                \"enforce_eager\": False, \"max_num_seqs\": 64,\n",
        "                \"limit_mm_per_prompt\": {\"image\": 16, \"video\": 8},\n",
        "                \"mm_processor_kwargs\": {\"min_pixels\": 784, \"max_pixels\": 2073600},\n",
        "                \"enable_prefix_caching\": True, \"enable_chunked_prefill\": True,\n",
        "            }\n",
        "        return {\n",
        "            \"model\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
        "            \"trust_remote_code\": True, \"dtype\": \"bfloat16\",\n",
        "            \"gpu_memory_utilization\": 0.95, \"max_model_len\": 32768,\n",
        "            \"enforce_eager\": False, \"max_num_seqs\": 64,\n",
        "            \"limit_mm_per_prompt\": {\"image\": 16, \"video\": 4},\n",
        "            \"mm_processor_kwargs\": {\"min_pixels\": 784, \"max_pixels\": 2073600},\n",
        "            \"enable_prefix_caching\": True, \"enable_chunked_prefill\": True,\n",
        "        }\n",
        "    \n",
        "    # B200 / BLACKWELL (SM 10.0, 192GB HBM3e)\n",
        "    else:\n",
        "        if prefer_qwen3:\n",
        "            return {\n",
        "                \"model\": \"Qwen/Qwen3-VL-8B-Instruct\",\n",
        "                \"trust_remote_code\": True, \"dtype\": \"bfloat16\",\n",
        "                \"gpu_memory_utilization\": 0.95, \"max_model_len\": 131072,  # 128K!\n",
        "                \"enforce_eager\": False, \"max_num_seqs\": 128,\n",
        "                \"limit_mm_per_prompt\": {\"image\": 32, \"video\": 16},\n",
        "                \"mm_processor_kwargs\": {\"min_pixels\": 784, \"max_pixels\": 4147200},\n",
        "                \"enable_prefix_caching\": True, \"enable_chunked_prefill\": True,\n",
        "            }\n",
        "        return {\n",
        "            \"model\": \"Qwen/Qwen2-VL-72B-Instruct\",  # Full 72B on single GPU!\n",
        "            \"trust_remote_code\": True, \"dtype\": \"bfloat16\",\n",
        "            \"gpu_memory_utilization\": 0.95, \"max_model_len\": 32768,\n",
        "            \"enforce_eager\": False, \"max_num_seqs\": 32,\n",
        "            \"limit_mm_per_prompt\": {\"image\": 16, \"video\": 8},\n",
        "            \"mm_processor_kwargs\": {\"min_pixels\": 784, \"max_pixels\": 4147200},\n",
        "            \"enable_prefix_caching\": True, \"enable_chunked_prefill\": True,\n",
        "        }\n",
        "\n",
        "# Select configuration - set PREFER_QWEN3 to False for Qwen2-VL\n",
        "PREFER_QWEN3 = True\n",
        "config = get_optimal_config(GPU_INFO[\"sm\"], GPU_INFO[\"memory\"], prefer_qwen3=PREFER_QWEN3)\n",
        "\n",
        "print(\"âš¡ SELECTED CONFIGURATION\")\n",
        "print(\"=\" * 80)\n",
        "for k, v in config.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Initialize vLLM Engine\n",
        "from vllm import LLM, SamplingParams\n",
        "import time\n",
        "\n",
        "print(\"âš¡ Initializing vLLM engine...\")\n",
        "print(f\"   Model: {config['model']}\")\n",
        "\n",
        "init_start = time.time()\n",
        "llm = LLM(**config)\n",
        "init_time = time.time() - init_start\n",
        "\n",
        "print(f\"\\nâœ… Engine initialized in {init_time:.1f}s\")\n",
        "\n",
        "allocated = torch.cuda.memory_allocated() / (1024**3)\n",
        "reserved = torch.cuda.memory_reserved() / (1024**3)\n",
        "print(f\"\\nğŸ“Š GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Create Test Image\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def create_test_image(width=1024, height=1024):\n",
        "    \"\"\"Create test dashboard image.\"\"\"\n",
        "    img = Image.new('RGB', (width, height), '#0f1420')\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    \n",
        "    # Top bar\n",
        "    draw.rectangle([0, 0, width, 60], fill='#1a2535')\n",
        "    draw.text((20, 20), \"Dashboard - Analytics\", fill='#ffffff')\n",
        "    \n",
        "    # Sidebar\n",
        "    draw.rectangle([0, 60, 200, height], fill='#141d2b')\n",
        "    items = [\"Dashboard\", \"Analytics\", \"Reports\", \"Settings\", \"Users\"]\n",
        "    for i, item in enumerate(items):\n",
        "        y = 100 + i * 50\n",
        "        draw.rectangle([10, y, 190, y + 40], fill='#1f2d42')\n",
        "        draw.text((30, y + 10), item, fill='#b0b0b0')\n",
        "    \n",
        "    # Cards\n",
        "    for row in range(2):\n",
        "        for col in range(3):\n",
        "            x, y = 220 + col * 260, 80 + row * 200\n",
        "            draw.rectangle([x, y, x + 240, y + 180], fill='#1a2535', outline='#2a3a4f')\n",
        "            draw.text((x + 20, y + 20), f\"Metric {row * 3 + col + 1}\", fill='#808080')\n",
        "            draw.text((x + 20, y + 60), f\"${(row * 3 + col + 1) * 1234:,}\", fill='#00d084')\n",
        "    \n",
        "    # Buttons\n",
        "    for i, (text, color) in enumerate([(\"Export\", '#3a7bd5'), (\"Refresh\", '#00a86b')]):\n",
        "        x = 220 + i * 150\n",
        "        draw.rectangle([x, height - 80, x + 130, height - 40], fill=color)\n",
        "        draw.text((x + 20, height - 70), text, fill='#ffffff')\n",
        "    \n",
        "    return img\n",
        "\n",
        "test_image = create_test_image()\n",
        "print(f\"ğŸ“¸ Created test image: {test_image.size}\")\n",
        "\n",
        "# Display thumbnail\n",
        "thumb = test_image.copy()\n",
        "thumb.thumbnail((600, 600))\n",
        "display(thumb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Prompt Format Function\n",
        "def format_prompt(instruction: str, model_name: str) -> str:\n",
        "    \"\"\"Format prompt based on model family.\"\"\"\n",
        "    system = \"You are a helpful visual assistant. Analyze images carefully.\"\n",
        "    \n",
        "    return (\n",
        "        f\"<|im_start|>system\\n{system}<|im_end|>\\n\"\n",
        "        f\"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>{instruction}<|im_end|>\\n\"\n",
        "        \"<|im_start|>assistant\\n\"\n",
        "    )\n",
        "\n",
        "print(\"âœ… Prompt functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Run Single Image Inference\n",
        "sampling_params = SamplingParams(temperature=0.0, max_tokens=512, stop=[\"<|im_end|>\"])\n",
        "\n",
        "instructions = [\n",
        "    \"Describe this dashboard interface.\",\n",
        "    \"What menu items are visible in the sidebar?\",\n",
        "    \"What is the total value of all metrics shown?\",\n",
        "]\n",
        "\n",
        "print(\"âš¡ Running inference...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "model_name = config[\"model\"]\n",
        "results = []\n",
        "\n",
        "for i, instruction in enumerate(instructions, 1):\n",
        "    prompt = format_prompt(instruction, model_name)\n",
        "    inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"image\": test_image}}\n",
        "    \n",
        "    start = time.time()\n",
        "    outputs = llm.generate([inputs], sampling_params=sampling_params)\n",
        "    latency = (time.time() - start) * 1000\n",
        "    \n",
        "    response = outputs[0].outputs[0].text\n",
        "    tokens = len(outputs[0].outputs[0].token_ids)\n",
        "    \n",
        "    print(f\"\\n[{i}] {instruction}\")\n",
        "    print(f\"    â±ï¸  Latency: {latency:.0f}ms | Tokens: {tokens} | Speed: {tokens/(latency/1000):.1f} tok/s\")\n",
        "    print(f\"    ğŸ“ Response: {response[:200]}...\" if len(response) > 200 else f\"    ğŸ“ Response: {response}\")\n",
        "    \n",
        "    results.append({\"instruction\": instruction, \"latency\": latency, \"tokens\": tokens})\n",
        "\n",
        "avg_latency = sum(r[\"latency\"] for r in results) / len(results)\n",
        "print(f\"\\nğŸ“Š Average latency: {avg_latency:.0f}ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Batch Inference (High Throughput)\n",
        "batch_instructions = [\n",
        "    \"Click on Dashboard\", \"Click on Analytics\", \"Click on Reports\",\n",
        "    \"Click on Settings\", \"Click on Users\", \"Click on Export\",\n",
        "    \"Click on Refresh\", \"Click on Metric 1\",\n",
        "]\n",
        "\n",
        "batch_inputs = [\n",
        "    {\"prompt\": format_prompt(inst, model_name), \"multi_modal_data\": {\"image\": test_image}}\n",
        "    for inst in batch_instructions\n",
        "]\n",
        "\n",
        "print(f\"âš¡ Batch inference: {len(batch_inputs)} requests\")\n",
        "\n",
        "batch_start = time.time()\n",
        "batch_outputs = llm.generate(batch_inputs, sampling_params=sampling_params)\n",
        "batch_time = (time.time() - batch_start) * 1000\n",
        "\n",
        "print(f\"\\nâœ… Completed in {batch_time:.0f}ms\")\n",
        "print(f\"ğŸ“ˆ Throughput: {len(batch_inputs) / (batch_time / 1000):.1f} requests/second\")\n",
        "print(f\"ğŸ“ˆ Per-request: {batch_time / len(batch_inputs):.0f}ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Architecture & GPU Summary\n",
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                    QWEN2-VL vs QWEN3-VL ARCHITECTURE COMPARISON                     â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘                                                                                     â•‘\n",
        "â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•‘\n",
        "â•‘  â”‚ Component            â”‚ Qwen2-VL                â”‚ Qwen3-VL                   â”‚   â•‘\n",
        "â•‘  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â•‘\n",
        "â•‘  â”‚ Patch Embedding      â”‚ Conv3D (no bias)        â”‚ Conv3D (WITH bias)         â”‚   â•‘\n",
        "â•‘  â”‚ Position Encoding    â”‚ 3D RoPE only            â”‚ Learned + RoPE + Interp    â”‚   â•‘\n",
        "â•‘  â”‚ MLP Activation       â”‚ QuickGELU               â”‚ SiLU                       â”‚   â•‘\n",
        "â•‘  â”‚ Multi-Scale Features â”‚ âŒ                      â”‚ âœ… DeepStack               â”‚   â•‘\n",
        "â•‘  â”‚ Video Token Pruning  â”‚ âŒ                      â”‚ âœ… EVS                     â”‚   â•‘\n",
        "â•‘  â”‚ Max Video Frames     â”‚ 14                      â”‚ 24,576                     â”‚   â•‘\n",
        "â•‘  â”‚ Speculative Decode   â”‚ Basic                   â”‚ âœ… Eagle3                  â”‚   â•‘\n",
        "â•‘  â”‚ MoE Variants         â”‚ âŒ                      â”‚ âœ… Qwen3-VL-30B-A3B        â”‚   â•‘\n",
        "â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘                          GPU PERFORMANCE EXPECTATIONS                               â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘                                                                                     â•‘\n",
        "â•‘  GPU          â”‚ Memory â”‚ Best Model             â”‚ Context â”‚ Latency â”‚ Throughput   â•‘\n",
        "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â•‘\n",
        "â•‘  T4           â”‚ 16 GB  â”‚ Qwen2-VL-2B            â”‚ 2K      â”‚ ~800ms  â”‚ ~20 tok/s    â•‘\n",
        "â•‘               â”‚        â”‚ Qwen3-VL-4B (4-bit)    â”‚ 2K      â”‚ ~1000ms â”‚ ~18 tok/s    â•‘\n",
        "â•‘  A100-80GB    â”‚ 80 GB  â”‚ Qwen3-VL-8B            â”‚ 16K     â”‚ ~250ms  â”‚ ~100 tok/s   â•‘\n",
        "â•‘  H100         â”‚ 80 GB  â”‚ Qwen3-VL-8B            â”‚ 32K     â”‚ ~150ms  â”‚ ~150 tok/s   â•‘\n",
        "â•‘               â”‚        â”‚ Qwen3-VL-30B-A3B (MoE) â”‚ 16K     â”‚ ~300ms  â”‚ ~60 tok/s    â•‘\n",
        "â•‘  B200         â”‚ 192 GB â”‚ Qwen3-VL-8B            â”‚ 128K    â”‚ ~80ms   â”‚ ~200 tok/s   â•‘\n",
        "â•‘               â”‚        â”‚ Qwen2-VL-72B           â”‚ 32K     â”‚ ~200ms  â”‚ ~80 tok/s    â•‘\n",
        "â•‘                                                                                     â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")\n",
        "\n",
        "print(\"âœ… Notebook complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
