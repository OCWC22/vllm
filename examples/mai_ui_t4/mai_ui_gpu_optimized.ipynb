{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ MAI-UI GUI Agent: GPU-Optimized vLLM Deployment\n",
        "\n",
        "## Complete Guide for T4, A100, H100, and B200\n",
        "\n",
        "This notebook demonstrates how to run [MAI-UI](https://www.alphaxiv.org/abs/2512.22047) - a state-of-the-art GUI agent built on Qwen3-VL - optimized for different NVIDIA GPUs.\n",
        "\n",
        "---\n",
        "\n",
        "### What is MAI-UI?\n",
        "\n",
        "MAI-UI (Multimodal AI UI) is a family of foundation GUI agents from Tongyi Lab (Alibaba) that:\n",
        "- Uses **Qwen3-VL** as its vision-language backbone\n",
        "- Trained with **GRPO** (Group Relative Policy Optimization) reinforcement learning\n",
        "- Achieves **76.7%** on AndroidWorld (SOTA) and **73.5%** on ScreenSpot-Pro\n",
        "- Supports device-cloud collaboration for privacy + accuracy\n",
        "\n",
        "### Why GPU-Specific Optimization Matters\n",
        "\n",
        "```\n",
        "GPU Memory ‚îÄ‚îÄ‚ñ∫ Model Size ‚îÄ‚îÄ‚ñ∫ Precision ‚îÄ‚îÄ‚ñ∫ Performance\n",
        "     ‚îÇ              ‚îÇ             ‚îÇ              ‚îÇ\n",
        "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         Wrong choices = OOM or 10x slower\n",
        "```\n",
        "\n",
        "| GPU | Key Constraint | Solution | Expected Latency |\n",
        "|-----|----------------|----------|------------------|\n",
        "| T4 (16GB) | Memory-limited | 4-bit quantization, small batch | ~1-2s/action |\n",
        "| A100 (80GB) | Bandwidth-limited | BF16, FlashAttention 2 | ~300-500ms |\n",
        "| H100 (80GB) | Compute-limited | FP8, FlashAttention 3 | ~150-300ms |\n",
        "| B200 (192GB) | None practical | Full model, max concurrency | ~100-200ms |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Architecture: Why Qwen3-VL Excels at GUI Tasks\n",
        "\n",
        "### DeepStack: Multi-Scale Visual Features\n",
        "\n",
        "GUI elements like buttons (20√ó20 pixels on 1080√ó2400 screen) are TINY. Standard vision encoders lose this detail.\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  DEEPSTACK: Qwen3-VL's Secret Weapon for GUI Agents                            ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  Vision Layer 8:  Edge detection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ LLM Layer 0  ‚îÇ\n",
        "‚îÇ                   (button borders, icon edges)                                 ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  Vision Layer 16: Shape patterns ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ LLM Layer 1  ‚îÇ\n",
        "‚îÇ                   (UI component shapes)                                        ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  Vision Layer 24: Widget structures ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ LLM Layer 2  ‚îÇ\n",
        "‚îÇ                   (navigation bars, dialogs)                                   ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  Vision Layer 32: Semantic meaning ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Embedding    ‚îÇ\n",
        "‚îÇ                   (\"Settings button\", \"Search field\")                          ‚îÇ\n",
        "‚îÇ                                                                                 ‚îÇ\n",
        "‚îÇ  RESULT: All scales available to LLM ‚Üí Better at finding small UI elements!   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### GRPO Training: How MAI-UI Learns\n",
        "\n",
        "```\n",
        "For each screenshot + task:\n",
        "  1. Sample 8 actions: [CLICK(100,200), CLICK(105,198), CLICK(500,300), ...]\n",
        "  2. Compute rewards: [1.0, 1.0, 0.0, 0.0, ...] (1.0 if inside target)\n",
        "  3. Normalize: advantages = (rewards - mean) / std\n",
        "  4. Update: increase probability of high-advantage actions\n",
        "\n",
        "WHY GRPO > PPO: No critic network ‚Üí Less memory ‚Üí Larger batches\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: GPU Detection and Auto-Configuration\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üîç GPU DETECTION AND AUTO-CONFIGURATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get GPU info\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        ['nvidia-smi', '--query-gpu=name,memory.total,compute_cap', '--format=csv,noheader'],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "    print(f\"\\nüìä GPU Info: {result.stdout.strip()}\")\n",
        "except:\n",
        "    print(\"‚ùå nvidia-smi not found!\")\n",
        "    sys.exit(1)\n",
        "\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"‚ùå CUDA not available!\")\n",
        "    sys.exit(1)\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "compute_cap = torch.cuda.get_device_capability(0)\n",
        "sm_version = compute_cap[0] * 10 + compute_cap[1]\n",
        "\n",
        "print(f\"\\n‚úÖ GPU: {gpu_name}\")\n",
        "print(f\"‚úÖ Memory: {gpu_memory_gb:.1f} GB\")\n",
        "print(f\"‚úÖ Compute Capability: SM {sm_version}\")\n",
        "\n",
        "# Determine GPU tier\n",
        "def get_gpu_tier():\n",
        "    \"\"\"Classify GPU into tier based on memory and compute capability.\"\"\"\n",
        "    if gpu_memory_gb > 150:\n",
        "        return 'B200'  # Blackwell, 192GB\n",
        "    elif sm_version >= 90:\n",
        "        return 'H100'  # Hopper, SM 9.0\n",
        "    elif sm_version >= 80:\n",
        "        return 'A100'  # Ampere, SM 8.0\n",
        "    else:\n",
        "        return 'T4'    # Turing or older\n",
        "\n",
        "GPU_TIER = get_gpu_tier()\n",
        "\n",
        "tier_info = {\n",
        "    'T4': {'emoji': 'üü°', 'features': ['FP16 only', 'TORCH_SDPA', '320 GB/s'], 'latency': '1-2s'},\n",
        "    'A100': {'emoji': 'üü¢', 'features': ['BF16', 'FlashAttn 2', '2,039 GB/s'], 'latency': '300-500ms'},\n",
        "    'H100': {'emoji': 'üîµ', 'features': ['FP8', 'FlashAttn 3', '3,350 GB/s'], 'latency': '150-300ms'},\n",
        "    'B200': {'emoji': 'üü£', 'features': ['192GB', 'FP8/FP4', '8,000 GB/s'], 'latency': '100-200ms'}\n",
        "}\n",
        "\n",
        "info = tier_info[GPU_TIER]\n",
        "print(f\"\\n{info['emoji']} Detected GPU Tier: {GPU_TIER}\")\n",
        "print(f\"   Features: {', '.join(info['features'])}\")\n",
        "print(f\"   Expected Latency: {info['latency']}\")\n",
        "\n",
        "# Install dependencies\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "%pip install -q vllm>=0.6.0 pillow requests jinja2\n",
        "print(\"‚úÖ Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: GPU-Optimized Configuration\n",
        "# Each config is tuned based on: GPU memory, compute capability, bandwidth\n",
        "\n",
        "def get_mai_ui_config(gpu_tier: str) -> dict:\n",
        "    \"\"\"\n",
        "    Get GPU-optimized vLLM configuration for MAI-UI.\n",
        "    \n",
        "    WHY DIFFERENT CONFIGS:\n",
        "    - T4: 16GB, no BF16 ‚Üí must use FP16 + small batch\n",
        "    - A100: 80GB, BF16 + FA2 ‚Üí full precision, medium batch\n",
        "    - H100: 80GB, FP8 + FA3 ‚Üí 2x compression, high batch\n",
        "    - B200: 192GB ‚Üí largest model, maximum batch\n",
        "    \"\"\"\n",
        "    configs = {\n",
        "        'T4': {\n",
        "            # T4: Memory-constrained, no BF16, no FlashAttention\n",
        "            'model': 'Tongyi-MAI/MAI-UI-2B',\n",
        "            'trust_remote_code': True,\n",
        "            'dtype': 'half',                    # FP16 only option\n",
        "            'gpu_memory_utilization': 0.92,\n",
        "            'max_model_len': 4096,\n",
        "            'enforce_eager': True,              # Save ~0.5GB\n",
        "            'max_num_seqs': 4,\n",
        "            'limit_mm_per_prompt': {'image': 2, 'video': 0},\n",
        "            'mm_processor_kwargs': {'min_pixels': 784, 'max_pixels': 512000},\n",
        "        },\n",
        "        'A100': {\n",
        "            # A100: BF16 full precision, FlashAttention 2\n",
        "            'model': 'Tongyi-MAI/MAI-UI-8B',\n",
        "            'trust_remote_code': True,\n",
        "            'dtype': 'bfloat16',\n",
        "            'gpu_memory_utilization': 0.95,\n",
        "            'max_model_len': 16384,\n",
        "            'enforce_eager': False,\n",
        "            'max_num_seqs': 16,\n",
        "            'limit_mm_per_prompt': {'image': 4, 'video': 1},\n",
        "            'mm_processor_kwargs': {'min_pixels': 784, 'max_pixels': 2073600, 'video_pruning_rate': 0.3},\n",
        "            'enable_prefix_caching': True,\n",
        "            'enable_chunked_prefill': True,\n",
        "        },\n",
        "        'H100': {\n",
        "            # H100: FP8 weights + KV cache, FlashAttention 3\n",
        "            'model': 'Tongyi-MAI/MAI-UI-8B',\n",
        "            'trust_remote_code': True,\n",
        "            'dtype': 'bfloat16',\n",
        "            'quantization': 'fp8',              # 2x smaller\n",
        "            'kv_cache_dtype': 'fp8',            # 2x smaller KV\n",
        "            'gpu_memory_utilization': 0.95,\n",
        "            'max_model_len': 32768,\n",
        "            'enforce_eager': False,\n",
        "            'max_num_seqs': 32,\n",
        "            'limit_mm_per_prompt': {'image': 8, 'video': 2},\n",
        "            'mm_processor_kwargs': {'min_pixels': 784, 'max_pixels': 2073600, 'video_pruning_rate': 0.3},\n",
        "            'enable_prefix_caching': True,\n",
        "            'enable_chunked_prefill': True,\n",
        "        },\n",
        "        'B200': {\n",
        "            # B200: Maximum everything, 192GB VRAM\n",
        "            'model': 'Tongyi-MAI/MAI-UI-32B',\n",
        "            'trust_remote_code': True,\n",
        "            'dtype': 'bfloat16',\n",
        "            'gpu_memory_utilization': 0.95,\n",
        "            'max_model_len': 65536,\n",
        "            'enforce_eager': False,\n",
        "            'max_num_seqs': 64,\n",
        "            'limit_mm_per_prompt': {'image': 16, 'video': 4},\n",
        "            'mm_processor_kwargs': {'min_pixels': 784, 'max_pixels': 4147200, 'video_pruning_rate': 0.2},\n",
        "            'enable_prefix_caching': True,\n",
        "            'enable_chunked_prefill': True,\n",
        "        },\n",
        "    }\n",
        "    return configs[gpu_tier]\n",
        "\n",
        "config = get_mai_ui_config(GPU_TIER)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"üìã CONFIGURATION FOR {GPU_TIER}\")\n",
        "print(\"=\" * 80)\n",
        "for key, value in config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Memory budget\n",
        "budgets = {\n",
        "    'T4': \"~11.5 GB / 16 GB used (‚úÖ Comfortable)\",\n",
        "    'A100': \"~44 GB / 80 GB used (‚úÖ Plenty of room)\",\n",
        "    'H100': \"~33 GB / 80 GB used (‚úÖ Room for 64+ concurrent)\",\n",
        "    'B200': \"~126 GB / 192 GB used (‚úÖ Massive headroom)\"\n",
        "}\n",
        "print(f\"\\nüìä Expected Memory: {budgets[GPU_TIER]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Initialize vLLM Engine\n",
        "from vllm import LLM, SamplingParams\n",
        "import time\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"üöÄ INITIALIZING vLLM ENGINE ({GPU_TIER})\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nüì• Loading model: {config['model']}\")\n",
        "print(\"   This may take several minutes on first run...\")\n",
        "\n",
        "init_start = time.time()\n",
        "llm = LLM(**config)\n",
        "init_time = time.time() - init_start\n",
        "\n",
        "print(f\"\\n‚úÖ Engine initialized in {init_time:.1f} seconds\")\n",
        "\n",
        "# Memory check\n",
        "allocated = torch.cuda.memory_allocated() / (1024**3)\n",
        "reserved = torch.cuda.memory_reserved() / (1024**3)\n",
        "print(f\"\\nüìä GPU Memory:\")\n",
        "print(f\"   Allocated: {allocated:.2f} GB\")\n",
        "print(f\"   Reserved:  {reserved:.2f} GB\")\n",
        "print(f\"   Free:      {gpu_memory_gb - reserved:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: MAI-UI Prompt Format and Parsing\n",
        "import re\n",
        "import json\n",
        "\n",
        "MAI_SYSTEM = \"\"\"You are MAI-UI, a GUI grounding agent. Given a screenshot and instruction, locate the UI element.\n",
        "Output: <grounding_think>[reasoning]</grounding_think><answer>{\"coordinate\": [x, y]}</answer>\n",
        "Coordinates: [0, 999] range where (0,0)=top-left, (999,999)=bottom-right.\"\"\"\n",
        "\n",
        "def build_prompt(instruction: str) -> str:\n",
        "    \"\"\"Build chat-formatted prompt with image placeholder.\"\"\"\n",
        "    return (\n",
        "        f\"<|im_start|>system\\n{MAI_SYSTEM}<|im_end|>\\n\"\n",
        "        f\"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\\n{instruction}<|im_end|>\\n\"\n",
        "        \"<|im_start|>assistant\\n\"\n",
        "    )\n",
        "\n",
        "def parse_response(text: str) -> dict:\n",
        "    \"\"\"Parse MAI-UI's structured response.\"\"\"\n",
        "    result = {'thinking': None, 'coordinate': None, 'raw': text}\n",
        "    \n",
        "    think = re.search(r\"<grounding_think>(.*?)</grounding_think>\", text, re.DOTALL)\n",
        "    if think:\n",
        "        result['thinking'] = think.group(1).strip()\n",
        "    \n",
        "    answer = re.search(r\"<answer>(.*?)</answer>\", text, re.DOTALL)\n",
        "    if answer:\n",
        "        try:\n",
        "            data = json.loads(answer.group(1).strip())\n",
        "            if 'coordinate' in data:\n",
        "                x, y = data['coordinate']\n",
        "                result['coordinate'] = [x / 999.0, y / 999.0]  # Normalize to [0, 1]\n",
        "        except:\n",
        "            pass\n",
        "    return result\n",
        "\n",
        "print(\"‚úÖ Prompt functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Create Test Screenshot\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def create_mobile_screenshot():\n",
        "    \"\"\"Create a realistic mobile settings screen (1080√ó1920).\"\"\"\n",
        "    img = Image.new('RGB', (1080, 1920), '#f5f5f5')\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    \n",
        "    # Status bar\n",
        "    draw.rectangle([0, 0, 1080, 80], fill='#1976D2')\n",
        "    draw.text((40, 30), \"9:41\", fill='white')\n",
        "    \n",
        "    # Header\n",
        "    draw.rectangle([0, 80, 1080, 200], fill='#2196F3')\n",
        "    draw.text((40, 120), \"Settings\", fill='white')\n",
        "    \n",
        "    # Settings items\n",
        "    items = [\n",
        "        ('Wi-Fi', True, 280), ('Bluetooth', True, 400), ('Cellular', True, 520),\n",
        "        ('Personal Hotspot', False, 640), ('VPN', False, 760), ('Notifications', None, 880),\n",
        "        ('Sounds & Haptics', None, 1000), ('Focus', None, 1120), ('Screen Time', None, 1240),\n",
        "        ('General', None, 1360),\n",
        "    ]\n",
        "    \n",
        "    for label, toggle, y in items:\n",
        "        draw.rectangle([0, y, 1080, y+100], fill='white', outline='#e0e0e0')\n",
        "        draw.text((40, y+35), label, fill='#333333')\n",
        "        if toggle is True:\n",
        "            draw.ellipse([960, y+30, 1020, y+70], fill='#4CAF50')\n",
        "        elif toggle is False:\n",
        "            draw.ellipse([960, y+30, 1020, y+70], fill='#9E9E9E')\n",
        "        draw.text((1020, y+35), \">\", fill='#999999')\n",
        "    \n",
        "    # Navigation bar\n",
        "    draw.rectangle([0, 1800, 1080, 1920], fill='white')\n",
        "    for i, label in enumerate(['Home', 'Search', 'Settings', 'Profile']):\n",
        "        draw.text((60 + i*270, 1840), label, fill='#2196F3' if label == 'Settings' else '#666666')\n",
        "    \n",
        "    return img\n",
        "\n",
        "test_image = create_mobile_screenshot()\n",
        "print(f\"üì∏ Created test screenshot: {test_image.size[0]}√ó{test_image.size[1]}\")\n",
        "\n",
        "# Display thumbnail\n",
        "thumb = test_image.copy()\n",
        "thumb.thumbnail((250, 450))\n",
        "display(thumb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Run MAI-UI Inference\n",
        "sampling_params = SamplingParams(temperature=0.0, max_tokens=512, stop=[\"<|im_end|>\"])\n",
        "\n",
        "instructions = [\n",
        "    \"Click on Wi-Fi to see network options\",\n",
        "    \"Tap the Bluetooth toggle\",\n",
        "    \"Open the General settings\",\n",
        "    \"Click the Home button in the navigation bar\",\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"ü§ñ RUNNING MAI-UI INFERENCE ({GPU_TIER})\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results = []\n",
        "total_start = time.time()\n",
        "\n",
        "for i, instruction in enumerate(instructions, 1):\n",
        "    inputs = {\"prompt\": build_prompt(instruction), \"multi_modal_data\": {\"image\": test_image}}\n",
        "    \n",
        "    start = time.time()\n",
        "    outputs = llm.generate([inputs], sampling_params=sampling_params)\n",
        "    latency_ms = (time.time() - start) * 1000\n",
        "    \n",
        "    parsed = parse_response(outputs[0].outputs[0].text)\n",
        "    results.append({'instruction': instruction, 'latency_ms': latency_ms, 'parsed': parsed})\n",
        "    \n",
        "    coord = parsed['coordinate']\n",
        "    if coord:\n",
        "        print(f\"\\n[{i}] {instruction}\")\n",
        "        print(f\"    üìç Coordinate: ({coord[0]:.3f}, {coord[1]:.3f}) = pixel ({int(coord[0]*1080)}, {int(coord[1]*1920)})\")\n",
        "        print(f\"    ‚è±Ô∏è  Latency: {latency_ms:.0f}ms\")\n",
        "    else:\n",
        "        print(f\"\\n[{i}] {instruction} ‚Üí ‚ùå No coordinate\")\n",
        "\n",
        "total_time = time.time() - total_start\n",
        "avg_latency = sum(r['latency_ms'] for r in results) / len(results)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä PERFORMANCE SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"  Total time: {total_time:.2f}s\")\n",
        "print(f\"  Average latency: {avg_latency:.0f}ms\")\n",
        "print(f\"  Throughput: {len(results) / total_time:.2f} req/s\")\n",
        "\n",
        "# Compare to expectations\n",
        "expected = {'T4': 1500, 'A100': 400, 'H100': 200, 'B200': 150}\n",
        "print(f\"\\n  Expected: ~{expected[GPU_TIER]}ms | Actual: {avg_latency:.0f}ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Visualize Results\n",
        "def visualize_predictions(image, results):\n",
        "    \"\"\"Overlay predicted click locations on screenshot.\"\"\"\n",
        "    img = image.copy()\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    colors = ['#FF0000', '#00FF00', '#0000FF', '#FF00FF', '#FFFF00']\n",
        "    \n",
        "    for i, result in enumerate(results):\n",
        "        coord = result['parsed'].get('coordinate')\n",
        "        if coord:\n",
        "            x, y = int(coord[0] * image.width), int(coord[1] * image.height)\n",
        "            color = colors[i % len(colors)]\n",
        "            # Crosshair\n",
        "            draw.ellipse([x-30, y-30, x+30, y+30], outline=color, width=4)\n",
        "            draw.line([x-40, y, x+40, y], fill=color, width=3)\n",
        "            draw.line([x, y-40, x, y+40], fill=color, width=3)\n",
        "            draw.text((x+35, y-15), str(i+1), fill=color)\n",
        "    return img\n",
        "\n",
        "vis_image = visualize_predictions(test_image, results)\n",
        "vis_thumb = vis_image.copy()\n",
        "vis_thumb.thumbnail((350, 600))\n",
        "display(vis_thumb)\n",
        "\n",
        "print(\"\\nüìç CLICK LOCATIONS:\")\n",
        "for i, r in enumerate(results, 1):\n",
        "    coord = r['parsed'].get('coordinate')\n",
        "    if coord:\n",
        "        print(f\"  [{i}] {r['instruction'][:40]}... ‚Üí ({int(coord[0]*1080)}, {int(coord[1]*1920)})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä GPU Optimization Summary\n",
        "\n",
        "### Why Each Parameter Matters\n",
        "\n",
        "| Parameter | What It Does | Impact of Wrong Choice |\n",
        "|-----------|--------------|------------------------|\n",
        "| `dtype` | Precision for weights | FP16 on A100+ = worse stability |\n",
        "| `quantization` | Compress weights | Full precision on T4 = OOM |\n",
        "| `enforce_eager` | Disable CUDA graphs | False on T4 = OOM |\n",
        "| `max_model_len` | Context window | Too large = OOM, too small = truncation |\n",
        "| `max_num_seqs` | Concurrent requests | Too large = OOM, too small = low throughput |\n",
        "| `max_pixels` | Max image resolution | Too large = many tokens = slow |\n",
        "| `enable_prefix_caching` | Cache system prompt | Critical for repeated prompts |\n",
        "\n",
        "### Memory Formula\n",
        "\n",
        "```\n",
        "Total Memory = Model Weights + KV Cache + Vision Encoder + Activations + Overhead\n",
        "\n",
        "Where:\n",
        "‚Ä¢ Model Weights = params √ó bytes_per_param (FP16=2, BF16=2, FP8=1, 4-bit=0.5)\n",
        "‚Ä¢ KV Cache = 2 √ó layers √ó heads √ó head_dim √ó max_len √ó max_seqs √ó bytes\n",
        "‚Ä¢ Vision Encoder = ~2-4 GB (depends on image resolution)\n",
        "‚Ä¢ Activations = ~1-5 GB (depends on batch size)\n",
        "```\n",
        "\n",
        "### References\n",
        "\n",
        "- [MAI-UI Technical Report](https://www.alphaxiv.org/abs/2512.22047)\n",
        "- [MAI-UI GitHub](https://github.com/Tongyi-MAI/MAI-UI)\n",
        "- [Qwen3-VL](https://github.com/QwenLM/Qwen3-VL)\n",
        "- [vLLM Documentation](https://docs.vllm.ai/)\n",
        "- [Complete Guide](QWEN_VL_COMPLETE_GUIDE.md)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
